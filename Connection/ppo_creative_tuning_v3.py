import os
import torch
import torch.nn as nn
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import PPOTrainer, PPOConfig, create_reference_model

# ==========================================
# 1. ä¿å­˜ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç®¡ç†ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ==========================================
def safe_save_model(model, tokenizer, path):
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆã—ã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜"""
    directory = os.path.dirname(path) if os.path.dirname(path) != "" else path
    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        print(f"ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {directory}")
    
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ã‚’ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚ï¼‰
    model.save_pretrained(path)
    tokenizer.save_pretrained(path)
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {path}")

# ==========================================
# 2. PPOè¨­å®šã¨ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
# ==========================================
model_name = "Qwen/Qwen2.5-7B-Instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"

config = PPOConfig(
    model_name=model_name,
    learning_rate=1.41e-5,
    batch_size=4,
    mini_batch_size=1,
    gradient_accumulation_steps=4,
    optimize_cuda_cache=True,
    early_stopping=True,
    target_kl=0.1,
)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ (ç”Ÿæˆç”¨ã«å·¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨­å®š)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.padding_side = 'left'
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (LoRAé©ç”¨)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    output_hidden_states=True
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
ref_model = create_reference_model(model) # å­¦ç¿’ã®åŸºæº–ç‚¹ã¨ãªã‚‹å‚ç…§ãƒ¢ãƒ‡ãƒ«

# ==========================================
# 3. å ±é…¬è¨ˆç®—ç”¨ãƒ—ãƒ­ãƒ¼ãƒ–ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# ==========================================
# å…ˆã»ã©å­¦ç¿’ã—ãŸ PooledCreativityProbe ã‚¯ãƒ©ã‚¹ã¯å®šç¾©æ¸ˆã¿ã¨ä»®å®š
# probe = load_creativity_probe("creativity_pooled_probe_v1.pth")
probe.eval()

def get_reward_from_probe(query_tensors, response_tensors):
    """
    ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ã‹ã‚‰å†…éƒ¨ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã€ãƒ—ãƒ­ãƒ¼ãƒ–ã§å ±é…¬ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚’è¡Œã†ã€‚
    å†…éƒ¨ç‰¹å¾´é‡ã®å·®åˆ†ã‚’åˆ©ç”¨ã—ã¦å‰µé€ æ€§ã‚’è©•ä¾¡ã™ã‚‹ã€‚
    """
    rewards = []
    # å„ã‚µãƒ³ãƒ—ãƒ«ã«ã¤ã„ã¦æ¨è«–ã‚’å›ã™
    for q, r in zip(query_tensors, response_tensors):
        full_input_ids = torch.cat([q, r], dim=-1).unsqueeze(0)
        
        with torch.no_grad():
            outputs = model.get_base_model()(**full_input_ids, output_hidden_states=True)
            
        # ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã®ç‰¹å®š
        gen_len = r.shape[0]
        pooled_layers = {}
        for l_idx in range(20, 27): # ç ”ç©¶ã§ç‰¹å®šã—ãŸã‚¹ã‚¤ãƒ¼ãƒˆã‚¹ãƒãƒƒãƒˆ
            # ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³å…¨ä½“ã®æ™‚é–“è»¸æ–¹å‘ã§ã®å¹³å‡ (Mean Pooling)
            step_vectors = [outputs.hidden_states[l_idx+1][:, -(gen_len - s), :] for s in range(gen_len)]
            pooled_layers[l_idx] = torch.stack(step_vectors).mean(dim=0).to(torch.float32)
            
        # ãƒ—ãƒ­ãƒ¼ãƒ–ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° (Logitã‚’ãã®ã¾ã¾å ±é…¬ã¨ã—ã¦åˆ©ç”¨)
        reward_logit = probe(pooled_layers)
        rewards.append(reward_logit.squeeze())
        
    return rewards

# ==========================================
# 4. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
# ==========================================
ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer)

# å­¦ç¿’ç”¨ã‚¯ã‚¨ãƒªï¼ˆåœ°ç„ã®æ¤œè¨¼ç­‰ã§ä½¿ç”¨ã—ãŸã‚ˆã†ãªå¤šæ§˜ãªãƒˆãƒ”ãƒƒã‚¯ï¼‰
dataset = ["æ¬¡ä¸–ä»£ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«ã¤ã„ã¦ã€æ¥µã‚ã¦ç‹¬å‰µçš„ãªæ¡ˆã‚’å‡ºã—ã¦ã€‚"] * 100 

print("ğŸš€ PPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
for epoch, batch in enumerate(tqdm(dataset)):
    query_tensors = [tokenizer.encode(q, return_tensors="pt").squeeze().to(device) for q in [batch]]
    
    # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç”Ÿæˆ
    generation_kwargs = {
        "min_length": -1,
        "top_k": 0.0,
        "top_p": 1.0,
        "do_sample": True,
        "pad_token_id": tokenizer.pad_token_id,
        "max_new_tokens": 32,
    }
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    
    # å ±é…¬ã®è¨ˆç®—
    rewards = get_reward_from_probe(query_tensors, response_tensors)
    
    # PPOã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œ
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    
    # 10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    if (epoch + 1) % 10 == 0:
        save_path = f"./checkpoints/ppo_creativity_model_step_{epoch+1}"
        safe_save_model(model, tokenizer, save_path)

# æœ€çµ‚ä¿å­˜
safe_save_model(model, tokenizer, "./final_creative_model_lora")
import os
import json
import torch
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer
from peft import LoraConfig
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model

# ---------------------------------------------------------
# 1. ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å®šç¾©ã—ãŸHybridProbeã‚¯ãƒ©ã‚¹ã®å†ç¾
# ---------------------------------------------------------
import torch
import torch.nn as nn
import torch.nn.functional as F

# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®è¨­å®šã¨åˆã‚ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
SEQ_LEN = 8 

class HybridProbe(nn.Module):
    def __init__(self, input_dim=3584, d_model=256):
        super().__init__()
        # 1. å…¥åŠ›å±¤ (Unexpected key: input_proj ã¨ä¸€è‡´)
        self.input_proj = nn.Linear(input_dim, d_model)
        
        # 2. Transformerå±¤ (num_layers=2 ã«ã™ã‚‹ã“ã¨ã§ layers.1 ãŒä½œæˆã•ã‚Œã¾ã™)
        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=4, batch_first=True)
        self.transformer = nn.TransformerEncoder(enc, num_layers=2)
        
        # 3. å‡ºåŠ›ãƒ˜ãƒƒãƒ‰ (Unexpected key: embed_head ã¨ä¸€è‡´)
        # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ•ãƒ©ãƒƒãƒˆã«ã—ã¦Linearã«å…¥åŠ›ã—ã¦ã„ã¾ã—ãŸ
        self.embed_head = nn.Linear(d_model * SEQ_LEN, 128)

    def forward(self, x):
        # x: [batch, seq_len, input_dim]
        x = self.input_proj(x)
        x = self.transformer(x)
        # ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã¦ãƒ˜ãƒƒãƒ‰ã¸
        x = x.reshape(x.size(0), -1)
        return F.normalize(self.embed_head(x), p=2, dim=1)

# ---------------------------------------------------------
# 2. PPOè¨­å®šã¨è³‡ç”£ã®ãƒ­ãƒ¼ãƒ‰
# ---------------------------------------------------------
SAVE_DIR = "creative_probe_final_v1"
with open(os.path.join(SAVE_DIR, "config.json"), "r") as f:
    cfg = json.load(f)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# PPOã®è¨­å®š
config = PPOConfig(
    model_name=cfg["model_name"],
    learning_rate=1e-5,
    batch_size=8,
    mini_batch_size=2,
    gradient_accumulation_steps=4,
    target_kl=0.06, # å†…éƒ¨ç‰¹å¾´é‡ã®æ€¥æ¿€ãªå¤‰åŒ–ã‚’é˜²ã
    optimize_cuda_cache=True,
)

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™
tokenizer = AutoTokenizer.from_pretrained(cfg["model_name"])
tokenizer.pad_token = tokenizer.eos_token

lora_config = LoraConfig(
    r=16, lora_alpha=32, target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05, bias="none", task_type="CAUSAL_LM"
)

model = AutoModelForCausalLMWithValueHead.from_pretrained(
    cfg["model_name"], peft_config=lora_config, device_map="auto", torch_dtype=torch.bfloat16
)
ref_model = create_reference_model(model)

# è³‡ç”£ã®ãƒ­ãƒ¼ãƒ‰
probe = HybridProbe(input_dim=cfg["input_dim"], d_model=cfg["d_model"]).to(DEVICE)
probe.load_state_dict(torch.load(os.path.join(SAVE_DIR, "probe_model.pt")))
probe.eval()
creative_prototype = torch.load(os.path.join(SAVE_DIR, "creative_prototype.pt")).to(DEVICE)

# ---------------------------------------------------------
# 3. å ±é…¬è¨ˆç®—é–¢æ•°ï¼ˆGated Reward Engineã®çµ±åˆï¼‰
# ---------------------------------------------------------
def get_external_reward(texts):
    """
    ã“ã“ã«DeBERTaãªã©ã®è«–ç†æ€§åˆ¤å®šãƒ¢ãƒ‡ãƒ«ã‚’é€£æºã•ã›ã¾ã™ã€‚
    ç¾åœ¨ã¯ç°¡æ˜“çš„ãªé•·ã•ãƒ»å½¢å¼ãƒã‚§ãƒƒã‚¯ã‚’ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã—ã¦ã„ã¾ã™ã€‚
    """
    # å®Ÿéš›ã®å®Ÿè£…ä¾‹: return reward_model(texts)
    return [1.0 if len(t) > 30 else -1.0 for t in texts]

def compute_rewards(queries, responses, model, tokenizer):
    rewards = []
    # ã‚²ãƒ¼ãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®è¨­å®šå€¤ã‚’åæ˜ ï¼‰
    tau = cfg.get("threshold_tau", 0.3)
    k = 1.0 / cfg.get("steepness_k", 15.0)

    for q, r in zip(queries, responses):
        # å†…éƒ¨ç‰¹å¾´é‡ã®æŠ½å‡ºï¼ˆç”Ÿæˆã•ã‚ŒãŸæ–‡å…¨ä½“ã§ã¯ãªãã€ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”éƒ¨åˆ†ã‚’é‡è¦–ï¼‰
        inputs = tokenizer(q + r, return_tensors="pt").to(DEVICE)
        with torch.no_grad():
            outputs = model.pretrained_model(**inputs, output_hidden_states=True)
            # æŒ‡å®šãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆ18å±¤ç›®ãªã©ï¼‰ã®éš ã‚ŒçŠ¶æ…‹ã‚’å–å¾—
            h = outputs.hidden_states[cfg["target_layer"]][0, -1:, :].float()
            # å†…ç™ºçš„å ±é…¬ï¼šãƒ—ãƒ­ãƒ¼ãƒ–ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã¨ã®é¡ä¼¼åº¦
            intrinsic = torch.nn.functional.cosine_similarity(probe(h), creative_prototype).item()

        # å¤–éƒ¨çš„å ±é…¬ï¼šè«–ç†æ€§
        ext_score = get_external_reward([r])[0]
        ext_norm = np.clip((ext_score + 2.0) / 4.0, 0, 1)

        # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã‚²ãƒ¼ãƒˆã®é©ç”¨
        gate = 1 / (1 + np.exp(-(ext_norm - tau) / k))
        
        # æœ€çµ‚å ±é…¬
        total_reward = torch.tensor(intrinsic * gate, dtype=torch.float32)
        rewards.append(total_reward)
    
    return rewards

# ---------------------------------------------------------
# 4. å­¦ç¿’ãƒ«ãƒ¼ãƒ—
# ---------------------------------------------------------
ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer)

print("ğŸš€ PPOå­¦ç¿’é–‹å§‹: å‰µé€ çš„ã‚µãƒ–ã‚¹ãƒšãƒ¼ã‚¹ã¸ã®æœ€é©åŒ–...")

for epoch in range(100):
    # ã‚¯ã‚¨ãƒªã®ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    query_txt = "æ–°ã—ã„ãƒã‚¤ã‚ªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®æ¦‚å¿µã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
    query_tensor = tokenizer.encode(query_txt, return_tensors="pt")[0]
    
    # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆï¼ˆç”Ÿæˆï¼‰
    response_tensors = ppo_trainer.generate(
        [query_tensor], max_new_tokens=64, do_sample=True, top_p=0.9
    )
    response_txt = [tokenizer.decode(r) for r in response_tensors]

    # å ±é…¬è¨ˆç®—
    rewards = compute_rewards([query_txt], response_txt, model, tokenizer)

    # PPOã‚¹ãƒ†ãƒƒãƒ—
    stats = ppo_trainer.step([query_tensor], response_tensors, rewards)
    
    # ãƒ­ã‚°å‡ºåŠ›
    ppo_trainer.log_stats(stats, {"query": query_txt, "response": response_txt[0]}, rewards[0])
    
    if epoch % 10 == 0:
        print(f"Epoch {epoch} | Reward: {rewards[0].item():.4f}")

# å­¦ç¿’æ¸ˆã¿LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ä¿å­˜
model.save_pretrained("qwen2.5_creative_ppo_final")
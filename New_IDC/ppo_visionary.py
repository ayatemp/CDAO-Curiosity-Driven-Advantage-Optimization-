import torch
import torch.nn.functional as F
from tqdm import tqdm
import wandb
import os
import numpy as np

from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from peft import LoraConfig, TaskType
from torch.utils.data import Dataset

# ==========================================
# ‚öôÔ∏è Configuration
# ==========================================
CONFIG = {
    "model_name": "Qwen/Qwen2.5-7B-Instruct",
    "vector_path": "probe_visionary_vector.pt", # „ÅÇ„Å™„Åü„ÅåÊäΩÂá∫„Åó„Åü„Éô„ÇØ„Éà„É´
    "target_layer": 16,     # ‰ªãÂÖ•„Åô„ÇãÂ±§
    "hidden_idx": 17,       # hidden_states„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ (Emb=0„Å™„ÅÆ„Åß+1)
    "learning_rate": 1.41e-5,
    "batch_size": 16,       # PPOÂÖ®‰Ωì„ÅÆ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫
    "mini_batch_size": 4,   # GPU„Å´‰πó„Çã„Çµ„Ç§„Ç∫ (ÂãæÈÖçËìÑÁ©ç„ÅßË™øÊï¥)
    "gradient_accumulation_steps": 1,
    "ppo_epochs": 4,
    "target_kl": 0.1,       # KLÂà∂Âæ°„ÅÆÁõÆÊ®ôÂÄ§
    "init_kl_coef": 0.2,
    "steps": 100,           # „Éá„É¢Áî®„Çπ„ÉÜ„ÉÉ„ÉóÊï∞ (ÈÅ©ÂÆúÂ¢ó„ÇÑ„Åó„Å¶„Åè„Å†„Åï„ÅÑ)
    "reward_scale": 15.0,   # Â†±ÈÖ¨„ÅÆÂÄçÁéá
    "wandb_project": "Visionary-PPO-Steering-LoRA",
    
    # --- LoRA Settings ---
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "lora_target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
}

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ==========================================
# üß† Reward Engine (Fixed: Float32 Cast)
# ==========================================
class LatentSteeringReward:
    def __init__(self, vector_path, target_hidden_idx, scale, device):
        print(f"Loading Steering Vector from {vector_path}...")
        # „Éô„ÇØ„Éà„É´„ÅØ float32 „Åß„É≠„Éº„Éâ
        self.vector = torch.load(vector_path).to(device).float()
        self.vector = F.normalize(self.vector, dim=0)
        self.target_hidden_idx = target_hidden_idx
        self.scale = scale
        self.device = device

    def compute_reward(self, model, input_ids, attention_mask, response_start_idx):
        # ValueHead‰ªò„Åç„É¢„Éá„É´„Åã„ÇâBase„É¢„Éá„É´(LoRAÈÅ©Áî®Ê∏à„Åø)„ÇíÂèñ„ÇäÂá∫„Åô
        base_model = model.pretrained_model
        
        # Êé®Ë´ñ„É¢„Éº„Éâ (ÂãæÈÖç‰∏çË¶Å)
        with torch.no_grad():
            outputs = base_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True
            )
        
        # ÊåáÂÆöÂ±§„ÅÆHidden StateÂèñÂæó
        target_h = outputs.hidden_states[self.target_hidden_idx]
        
        # ‚òÖ‰øÆÊ≠£: Âº∑Âà∂ÁöÑ„Å´Float32„Å´„Ç≠„É£„Çπ„Éà„Åó„Å¶Ë®àÁÆó (Half„Ç®„É©„ÉºÂõûÈÅø)
        target_h = target_h.float()
        
        # Ê≠£Ë¶èÂåñ
        target_h_norm = F.normalize(target_h, dim=-1)
        
        # „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶
        similarity = torch.matmul(target_h_norm, self.vector)
        
        rewards = []
        batch_size = input_ids.shape[0]
        
        for i in range(batch_size):
            # „É¨„Çπ„Éù„É≥„ÇπÈÉ®ÂàÜ„ÅÆ„ÅøÊäΩÂá∫
            resp_sim = similarity[i, response_start_idx[i]:] 
            
            if len(resp_sim) > 0:
                score = resp_sim.mean().item()
            else:
                score = 0.0
            
            rewards.append(score * self.scale)
            
        return rewards

# ==========================================
# üìö Creative Dataset
# ==========================================
class VisionaryDataset(Dataset):
    def __init__(self, tokenizer, num_samples=500):
        self.tokenizer = tokenizer
        # „Éô„ÇØ„Éà„É´„ÅåÂèçÂøú„Åó„ÇÑ„Åô„ÅÑÂâµÈÄ†ÁöÑ„Å™„ÅäÈ°å
        base_prompts = [
            "Propose a radical new technology to manipulate gravity.",
            "Theorize a biological mechanism for immortality.",
            "Explain how consciousness arises from quantum effects.",
            "Invent a device to record dreams.",
            "What exists outside of time?",
            "Is the universe a simulation? Argue for yes.",
            "Define 'justice' for an alien civilization.",
            "Describe a color that implies sadness.",
            "Write a myth about the death of the sun.",
            "Describe the sound of silence in a crowded room.",
            "A poem about a clock that counts backwards.",
            "The diary entry of the last human on Earth.",
            "Describe a city built entirely of glass.",
            "Explain the concept of infinity to a child using metaphors.",
            "Design a new organ for humans to survive on Mars."
        ]
        
        self.inputs = []
        print("Building dataset...")
        # „Éá„Éº„ÇøÂ¢óÂπÖ
        for _ in range(num_samples // len(base_prompts) + 1):
            for p in base_prompts:
                # „Ç∑„É≥„Éó„É´„Å™„É¶„Éº„Ç∂„Éº„Éó„É≠„É≥„Éó„Éà
                msgs = [{"role": "user", "content": p}]
                txt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
                self.inputs.append(txt)
                
        self.inputs = self.inputs[:num_samples]

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        return self.inputs[idx]

def collator(data):
    return dict(query=data)

# ==========================================
# üèÉ‚Äç‚ôÇÔ∏è Training Loop with LoRA
# ==========================================
def train():
    wandb.init(project=CONFIG["wandb_project"], config=CONFIG)
    
    # 1. Tokenizer
    print("Loading Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(CONFIG["model_name"])
    tokenizer.pad_token = tokenizer.eos_token 

    # 2. LoRA Config
    print("Configuring LoRA...")
    lora_config = LoraConfig(
        r=CONFIG["lora_r"],
        lora_alpha=CONFIG["lora_alpha"],
        lora_dropout=CONFIG["lora_dropout"],
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=CONFIG["lora_target_modules"]
    )

    # 3. Load Model with LoRA
    print("Loading Model with Adapter...")
    # TRL„ÅÆAutoModelForCausalLMWithValueHead„ÅØ„ÄÅpeft_config„ÇíÊ∏°„Åô„Å®Ëá™ÂãïÁöÑ„Å´LoRA„É¢„Éá„É´„Çí‰Ωú„Å£„Å¶„Åè„Çå„Åæ„Åô
    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        CONFIG["model_name"],
        peft_config=lora_config,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    # 4. PPO Config
    # ref_model=None „Å´„Åô„Çã„Å®„ÄÅTRL„ÅØËá™ÂãïÁöÑ„Å´„Äå„Ç¢„ÉÄ„Éó„Çø„Éº„ÇíÁÑ°ÂäπÂåñ„Åó„ÅüÁä∂ÊÖã„Äç„ÇíRef„Å®„Åó„Å¶Êâ±„ÅÑ„Åæ„Åô(„É°„É¢„É™ÁØÄÁ¥Ñ)
    ppo_config = PPOConfig(
        model_name=CONFIG["model_name"],
        learning_rate=CONFIG["learning_rate"],
        batch_size=CONFIG["batch_size"],
        mini_batch_size=CONFIG["mini_batch_size"],
        gradient_accumulation_steps=CONFIG["gradient_accumulation_steps"],
        ppo_epochs=CONFIG["ppo_epochs"],
        target_kl=CONFIG["target_kl"],
        init_kl_coef=CONFIG["init_kl_coef"],
        remove_unused_columns=False
    )
    
    # 5. Prepare Dataset
    dataset = VisionaryDataset(tokenizer, num_samples=CONFIG["steps"] * CONFIG["batch_size"])
    
    # 6. Initialize Trainer
    ppo_trainer = PPOTrainer(
        config=ppo_config,
        model=model,
        ref_model=None, # LoRA„ÅÆÂ†¥Âêà„ÅØNoneÊé®Â•®
        tokenizer=tokenizer,
        dataset=dataset,
        data_collator=collator
    )
    
    # 7. Reward Engine
    reward_engine = LatentSteeringReward(
        CONFIG["vector_path"], 
        CONFIG["hidden_idx"], 
        CONFIG["reward_scale"], 
        DEVICE
    )
    
    print("Starting LoRA-PPO Training...")
    
    # --- Loop ---
    for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):
        if step >= CONFIG["steps"]: break
        
        queries = batch["query"]
        query_tensors = [tokenizer(q, return_tensors="pt").input_ids.squeeze().to(DEVICE) for q in queries]
        
        # A. Rollout (Generate)
        response_tensors = ppo_trainer.generate(
            query_tensors,
            return_prompt=False,
            max_new_tokens=64,
            temperature=0.9,
            top_p=0.95,
            do_sample=True
        )
        
        batch["response"] = tokenizer.batch_decode(response_tensors)
        
        # B. Reward Calculation
        rewards = []
        full_input_ids = []
        masks = []
        response_start_indices = []
        
        for q_t, r_t in zip(query_tensors, response_tensors):
            full = torch.cat((q_t, r_t))
            full_input_ids.append(full)
            masks.append(torch.ones_like(full))
            response_start_indices.append(len(q_t))
            
        full_input_ids_tensor = torch.nn.utils.rnn.pad_sequence(
            full_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id
        ).to(DEVICE)
        
        attention_mask_tensor = torch.nn.utils.rnn.pad_sequence(
            masks, batch_first=True, padding_value=0
        ).to(DEVICE)
        
        raw_rewards = reward_engine.compute_reward(
            ppo_trainer.model, 
            full_input_ids_tensor, 
            attention_mask_tensor,
            response_start_indices
        )
        
        # List of tensors for TRL
        rewards = [torch.tensor(r).to(DEVICE) for r in raw_rewards]
        
        # C. PPO Step
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        
        # D. Logging
        mean_reward = np.mean(raw_rewards)
        
        log_data = {
            "step": step,
            "visionary_reward": mean_reward,
            "ppo/learning_rate": stats["ppo/learning_rate"],
            "env/kl_mean": stats["objective/kl"],
        }
        
        if step % 5 == 0:
            table = wandb.Table(columns=["Query", "Response", "Reward"])
            # ÂÖàÈ†≠2„Å§„Å†„Åë„É≠„Ç∞
            for q, r, rew in zip(queries[:2], batch["response"][:2], raw_rewards[:2]):
                table.add_data(q, r, rew)
            log_data["generated_samples"] = table
            
        wandb.log(log_data)
        
    print("Training Complete!")
    
    # Save LoRA Adapter
    print("Saving LoRA adapters...")
    ppo_trainer.model.save_pretrained("Qwen-Visionary-LoRA")
    tokenizer.save_pretrained("Qwen-Visionary-LoRA")

if __name__ == "__main__":
    train()
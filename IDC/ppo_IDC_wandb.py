#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ppo_transformer_probe_hybrid.py

Hybrid Reward PPO:
 - Policy: Qwen/Qwen2.5-7B-Instruct (with LoRA)
 - Extrinsic Reward: DeBERTa (Quality/Safety)
 - Intrinsic Reward: Transformer Probe (Creativity/Context-Aware)
 - Strategy: Gated Hybrid (Quality Check -> Creativity Bonus)

Usage:
python ppo_transformer_probe_hybrid.py \
    --probe-path "transformer_creativity_probe.pt" \
    --wandb-run-name "run-creative-probe-v1" \
    --w-ext 1.0 \
    --w-int 0.3 \
    --gate-threshold -1.5
"""

import argparse
import random
import textwrap
import numpy as np
import torch
import torch.nn as nn
import wandb
import os
import time
from tqdm import tqdm

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
from trl import (
    PPOConfig,
    PPOTrainer,
    AutoModelForCausalLMWithValueHead,
    create_reference_model,
)
from peft import LoraConfig

# ============================================================
# 1. Probe Model Architecture (Must match saved model)
# ============================================================
class TransformerProbe(nn.Module):
    def __init__(self, input_dim, d_model=256, nhead=4):
        super().__init__()
        self.project = nn.Linear(input_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=512, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)
        self.head = nn.Linear(d_model, 1)
        
    def forward(self, x):
        # x: [Batch, Seq, Input_Dim]
        x = self.project(x)
        x = self.transformer(x)
        # Global Average Pooling
        x = x.mean(dim=1) 
        return self.head(x)

class TransformerProbeRewardModel(nn.Module):
    """
    ä¿å­˜ã•ã‚ŒãŸProbeã‚’èª­ã¿è¾¼ã¿ã€å ±é…¬ï¼ˆLogitï¼‰ã‚’è¨ˆç®—ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼
    """
    def __init__(self, path: str, device: torch.device):
        super().__init__()
        self.device = device
        
        try:
            checkpoint = torch.load(path, map_location=device)
            config = checkpoint["config"]
            
            self.layer_idx = config["layer_idx"]
            self.model = TransformerProbe(
                input_dim=config["input_dim"],
                d_model=config["d_model"],
                nhead=config["nhead"]
            ).to(device)
            
            self.model.load_state_dict(checkpoint["model_state_dict"])
            self.model.eval()
            
            # PPOä¸­ã«ProbeãŒæ›´æ–°ã•ã‚Œãªã„ã‚ˆã†ã«å‡çµ
            for param in self.model.parameters():
                param.requires_grad = False
                
            print(f"[INFO] Probe Loaded: Layer {self.layer_idx}, Dim {config['input_dim']}")
            
        except Exception as e:
            print(f"[ERROR] Failed to load probe '{path}': {e}")
            raise e

    def get_reward(self, hidden_states: torch.Tensor) -> float:
        """
        hidden_states: [Seq, Dim] (1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹)
        """
        with torch.no_grad():
            # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ  [1, Seq, Dim]
            x = hidden_states.unsqueeze(0).to(self.device)
            logit = self.model(x).item()
        return logit

# ============================================================
# 2. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def shorten(text: str, width: int = 256) -> str:
    text = text.replace("\n", " ")
    return textwrap.shorten(text, width=width, placeholder="...")

# ============================================================
# 3. å ±é…¬è¨ˆç®—é–¢æ•°
# ============================================================
@torch.no_grad()
def compute_external_rm_rewards(
    rm_model, rm_tokenizer,
    problems, responses,
    device,
    max_length=512
):
    """å¤–éƒ¨ RM (DeBERTa) ã®å ±é…¬ã‚’è¨ˆç®— (Quality)"""
    inputs = []
    for p, r in zip(problems, responses):
        # Chatå½¢å¼ã‚„Instructå½¢å¼ã«åˆã‚ã›ã¦èª¿æ•´
        txt = f"User: {shorten(p)}\nAssistant: {r}"
        inputs.append(txt)
        
    enc = rm_tokenizer(inputs, return_tensors="pt", padding=True, truncation=True, max_length=max_length).to(device)
    out = rm_model(**enc)
    
    # Logits (é€šå¸¸ã¯ [Batch, 1] ã¾ãŸã¯ [Batch, 2])
    if out.logits.shape[-1] == 1:
        return out.logits.squeeze(-1).tolist()
    else:
        return out.logits[:, 0].tolist()

@torch.no_grad()
def compute_internal_probe_rewards(
    policy_model, tokenizer,
    prompts, response_tensors,
    probe_wrapper
):
    """
    ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®HiddenStatesã‚’å–å¾—ã—ã€Probeã§å‰µé€ æ€§ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã™ã‚‹
    """
    rewards = []
    device = policy_model.pretrained_model.device
    
    # Tensor -> Text
    responses_text = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)
    
    for p, r_text in zip(prompts, responses_text):
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ+å¿œç­”ã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã¦Hidden Stateã‚’å–å¾—
        # â€»ç”Ÿæˆæ™‚ã¨åŒã˜çŠ¶æ…‹ã‚’å†ç¾
        full_text = p + r_text
        inp = tokenizer(full_text, return_tensors="pt").to(device)
        
        # Forward Pass (Gradientä¸è¦)
        # policy_modelã¯AutoModelForCausalLMWithValueHeadãªã®ã§ã€.pretrained_modelã«ã‚¢ã‚¯ã‚»ã‚¹
        out = policy_model.pretrained_model(**inp, output_hidden_states=True)
        
        # æŒ‡å®šå±¤ã®Hidden Stateã‚’å–å¾— [1, Seq, Dim] -> [Seq, Dim]
        # target_layer_idx ã¯ Probeå­¦ç¿’æ™‚ã¨åŒã˜ã‚‚ã®ã‚’ä½¿ã†
        target_layer = probe_wrapper.layer_idx
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã‚’å–å¾— (ProbeãŒTransformerãªã®ã§æ–‡è„ˆãŒå¿…è¦)
        h_seq = out.hidden_states[target_layer].squeeze(0).float()
        
        # Probeæ¨è«–
        score = probe_wrapper.get_reward(h_seq)
        rewards.append(score)
        
    return rewards

# ============================================================
# 4. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================
def main():
    parser = argparse.ArgumentParser()

    # Models
    parser.add_argument("--policy-model-name", type=str, default="Qwen/Qwen2.5-7B-Instruct")
    parser.add_argument("--rm-model-name", type=str, default="OpenAssistant/reward-model-deberta-v3-large-v2")
    parser.add_argument("--probe-path", type=str, default="transformer_creativity_probe.pt", help="Path to saved probe")

    # PPO Params
    parser.add_argument("--num-steps", type=int, default=200, help="Total optimization steps")
    parser.add_argument("--batch-size", type=int, default=16)
    parser.add_argument("--mini-batch-size", type=int, default=4)
    parser.add_argument("--max-new-tokens", type=int, default=128)
    parser.add_argument("--lr", type=float, default=1.41e-5)
    parser.add_argument("--init-kl-coef", type=float, default=0.1)
    parser.add_argument("--seed", type=int, default=42)

    # Hybrid Reward Params
    parser.add_argument("--w-ext", type=float, default=1.0, help="å¤–éƒ¨å ±é…¬(å“è³ª)ã®é‡ã¿")
    parser.add_argument("--w-int", type=float, default=0.3, help="å†…éƒ¨å ±é…¬(å‰µé€ æ€§)ã®é‡ã¿")
    parser.add_argument("--gate-threshold", type=float, default=-1.5, help="è¶³åˆ‡ã‚Šãƒ©ã‚¤ãƒ³ã€‚ã“ã‚Œä»¥ä¸‹ã®å“è³ªãªã‚‰å‰µé€ æ€§ãƒœãƒ¼ãƒŠã‚¹ç„¡åŠ¹")

    # W&B
    parser.add_argument("--wandb-project", type=str, default="qwen-creative-ppo")
    parser.add_argument("--wandb-run-name", type=str, default=None)

    args = parser.parse_args()
    set_seed(args.seed)

    # --------------------------------------------------------
    # PPO Configuration
    # --------------------------------------------------------
    ppo_config = PPOConfig(
        model_name=args.policy_model_name,
        learning_rate=args.lr,
        batch_size=args.batch_size,
        mini_batch_size=args.mini_batch_size,
        gradient_accumulation_steps=1,
        optimize_cuda_cache=True,
        target_kl=0.1,
        init_kl_coef=args.init_kl_coef,
        ppo_epochs=4,
        seed=args.seed,
        log_with="wandb",
        tracker_project_name=args.wandb_project,
        tracker_kwargs={"wandb": {"name": args.wandb_run_name}}
    )

    # --------------------------------------------------------
    # Model Loading (with LoRA)
    # --------------------------------------------------------
    print(f"[INFO] Loading Policy Model: {args.policy_model_name}")
    
    # LoRA Config (ãƒ¡ãƒ¢ãƒªç¯€ç´„ & åŠ¹ç‡çš„å­¦ç¿’)
    lora_config = LoraConfig(
        r=16, 
        lora_alpha=32, 
        lora_dropout=0.05, 
        bias="none", 
        task_type="CAUSAL_LM"
    )

    # Policy Model
    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        ppo_config.model_name,
        torch_dtype=torch.float16,
        peft_config=lora_config,
        device_map="auto"
    )
    
    # Reference Model (LoRAã®å ´åˆã¯å…±æœ‰ã•ã‚Œã‚‹ãŒã€trlã®ä»•æ§˜ä¸Šä½œæˆ)
    ref_model = create_reference_model(model)
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained(ppo_config.model_name)
    # pad_tokenãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯eosã¨åŒã˜å ´åˆã®å®‰å…¨ç­–
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token # ä¸€æ—¦EOSã«ã™ã‚‹ãŒ...
        
    # â˜…é‡è¦: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æ–¹å‘ã‚’å·¦ã«ã™ã‚‹ (ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã§ã¯å·¦è©°ã‚ãŒé‰„å‰‡)
    tokenizer.padding_side = "left"

    # Trainer Initialization
    # (Datasetã¯å¾Œã§å‹•çš„ã«ä¸ãˆã‚‹ãŸã‚ã€ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ã‚ã‚‹ã„ã¯Noneã§ã‚‚å¯ã ãŒã€
    #  trlã®ä»•æ§˜ä¸Šã€åˆæœŸåŒ–æ™‚ã«tokenizerç­‰ã‚’æ¸¡ã™ã®ãŒä¸€èˆ¬çš„)
    # ã“ã“ã§ã¯ä¾¿å®œä¸Šã€å¾Œã§ãƒ«ãƒ¼ãƒ—å†…ã§ batch ã‚’ä½œæˆã™ã‚‹ãŸã‚ã€ç©ºã§åˆæœŸåŒ–ç­‰ã¯ã›ãšã€
    # ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ—æ§‹ç¯‰ã«å¾“ã„ã¾ã™ã€‚
    
    # Note: trl.PPOTrainer ã¯ dataset ã‚’å¼•æ•°ã«å–ã‚Šã¾ã™ãŒã€
    # ã“ã“ã§ã¯ç‹¬è‡ªã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒ¼ãƒ«ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã€
    # æ‰‹å‹•ã§ãƒãƒƒãƒã‚’ä½œæˆã—ã¦ trainer.step ã«æ¸¡ã™ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã¨ã‚Šã¾ã™ã€‚
    # ãã®ãŸã‚ã€ãƒ€ãƒŸãƒ¼ã®datasetã§åˆæœŸåŒ–ã—ã¾ã™ã€‚
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ (Research Ideas)
    base_inst = (
        "You are an expert researcher. Propose a novel and concrete research idea.\n"
        "Output ONLY in the following format:\n\n"
        "Title: <concise title>\n"
        "Abstract: <150-200 word abstract>\n"
    )
    topics = [
        "Mixture of Experts", "State Space Models", "Sparse Attention",
        "KV-Cache Optimization", "Continual Learning", "Federated Learning",
        "Quantization", "Knowledge Distillation", "LoRA", "RLHF", "DPO", 
        "Chain of Thought", "Multi-Agent", "Tool use", "RAG", 
        "Synthetic data", "Multilingual"
    ]
    perspectives = ["efficiency", "interpretability", "robustness", "reasoning", "cost"]
    
    prompt_list = []
    for t in topics:
        for p in perspectives:
            txt = f"Draft a research proposal about {t} focusing on {p}.\n{base_inst}"
            prompt_list.append(txt)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ä½œæˆ (Trainerã«æ¸¡ã™ãŸã‚)
    import datasets
    dataset = datasets.Dataset.from_dict({"query": prompt_list})
    
    def collator(data):
        return {key: [d[key] for d in data] for key in data[0]}

    trainer = PPOTrainer(
        config=ppo_config,
        model=model,
        ref_model=ref_model,
        tokenizer=tokenizer,
        dataset=dataset,
        data_collator=collator
    )
    
    device = trainer.accelerator.device

    # --------------------------------------------------------
    # Reward Models Loading
    # --------------------------------------------------------
    print(f"[INFO] Loading Extrinsic RM: {args.rm_model_name}")
    rm_tokenizer = AutoTokenizer.from_pretrained(args.rm_model_name)
    rm_model = AutoModelForSequenceClassification.from_pretrained(
        args.rm_model_name, torch_dtype=torch.float16
    ).to(device)
    rm_model.eval()

    print(f"[INFO] Loading Intrinsic Probe: {args.probe_path}")
    probe_wrapper = TransformerProbeRewardModel(args.probe_path, device)

    # --------------------------------------------------------
    # Training Loop
    # --------------------------------------------------------
    print(f"[INFO] Starting Training for {args.num_steps} steps...")
    
    # DataLoaderã‹ã‚‰ç„¡é™ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿åŒ–
    data_iter = iter(trainer.dataloader)

    for step in tqdm(range(args.num_steps)):
        # ãƒ‡ãƒ¼ã‚¿ã®å–å¾— (ãªããªã£ãŸã‚‰å†åˆæœŸåŒ–)
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(trainer.dataloader)
            batch = next(data_iter)

        query_tensors = []
        response_tensors = []
        
        # --- 1. Generation (Rollout) ---
        # ãƒãƒƒãƒå†…ã®ã‚¯ã‚¨ãƒªã”ã¨ã«ç”Ÿæˆ
        for query in batch["query"]:
            inputs = tokenizer(query, return_tensors="pt").to(DEVICE)
            query_tensor = inputs.input_ids.squeeze(0)
            attention_mask = inputs.attention_mask # ã“ã‚Œã‚’å–å¾—ï¼
            
            generation_kwargs = {
                "min_length": -1,
                "top_k": 0.0,
                "top_p": 1.0,
                "do_sample": True,
                "pad_token_id": tokenizer.eos_token_id,
                "max_new_tokens": args.max_new_tokens,
                "temperature": 0.95, # å‰µé€ æ€§ã®ãŸã‚ã«å°‘ã—é«˜ã‚
                "attention_mask": attention_mask.to(DEVICE)
            }
            
            # generate
            response = trainer.generate(query_tensor, **generation_kwargs).squeeze(0)
            # promptéƒ¨åˆ†ã‚’é™¤å»ã—ã¦ response ã®ã¿ã«ã™ã‚‹
            response_tensors.append(response[len(query_tensor):])

        # --- 2. Reward Calculation ---
        batch_responses = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)
        
        # A. Extrinsic (Quality)
        ext_scores = compute_external_rm_rewards(
            rm_model, rm_tokenizer, batch["query"], batch_responses, device
        )
        
        # B. Intrinsic (Creativity Probe)
        int_scores = compute_internal_probe_rewards(
            trainer.model, tokenizer, batch["query"], response_tensors, probe_wrapper
        )

        # C. Hybrid & Gating
        final_rewards = []
        stats_ext = []
        stats_int = []
        gated_count = 0
        texts_for_log = []

        for q, r, ext, int_val in zip(batch["query"], batch_responses, ext_scores, int_scores):
            # è¶³åˆ‡ã‚Šãƒ­ã‚¸ãƒƒã‚¯
            if ext < args.gate_threshold:
                # å“è³ªãŒæ‚ªã™ãã‚‹ -> å‰µé€ æ€§ãƒœãƒ¼ãƒŠã‚¹ãªã— (ç½°ã®ã¿)
                total = float(ext)
                gated_count += 1
            else:
                # å“è³ªOK -> ãƒœãƒ¼ãƒŠã‚¹åŠ ç®—
                total = float(ext) + (args.w_int * float(int_val))
            
            final_rewards.append(torch.tensor(total, device=device))
            
            stats_ext.append(ext)
            stats_int.append(int_val)
            texts_for_log.append([q, r, total, ext, int_val])

        # --- 3. PPO Update ---
        stats = trainer.step(query_tensors, response_tensors, final_rewards)

        # --- 4. Logging ---
        # æ•°å€¤ãƒ­ã‚°
        stats["env/reward_mean_total"] = np.mean([r.item() for r in final_rewards])
        stats["env/reward_mean_ext"] = np.mean(stats_ext)
        stats["env/reward_mean_int"] = np.mean(stats_int)
        stats["env/gated_ratio"] = gated_count / len(batch["query"])
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ­ã‚° (W&B Table) - 10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨
        if step % 10 == 0 and wandb.run is not None:
            table = wandb.Table(
                columns=["Query", "Response", "Total", "Ext(Qual)", "Int(Creat)"],
                data=texts_for_log
            )
            wandb.log({"game_log": table}, step=step)

        batch["response"] = batch_responses
        trainer.log_stats(stats, batch, final_rewards)
        
        # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«é€²æ—è¡¨ç¤º
        if step % 5 == 0:
            print(f"\n[Step {step}] Total: {stats['env/reward_mean_total']:.2f} | "
                  f"Ext: {np.mean(stats_ext):.2f} | Int: {np.mean(stats_int):.2f}")

    # --------------------------------------------------------
    # Save Model
    # --------------------------------------------------------
    save_dir = f"./saved_models/{args.wandb_run_name}" if args.wandb_run_name else "./saved_models/ppo_final"
    print(f"\n[INFO] Saving model to {save_dir} ...")
    trainer.model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    
    wandb.finish()
    
    # çµ‚äº†é€šçŸ¥éŸ³
    print("ğŸ”” å­¦ç¿’ãŒçµ‚äº†ã—ã¾ã—ãŸï¼")
    for _ in range(3):
        print('\a')
        time.sleep(0.5)

if __name__ == "__main__":
    main()
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "find_best_layer_strict.py\n",
    "\n",
    "å¤‰æ›´ç‚¹:\n",
    " - ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’2000ã«å¢—åŠ \n",
    " - Hard Negativeï¼ˆå°‚é–€ç”¨èªã®ç¾…åˆ—ãªã©ï¼‰ã‚’è¿½åŠ ã—ã¦é›£æ˜“åº¦ã‚¢ãƒƒãƒ—\n",
    " - 5-Fold Cross-Validation (äº¤å·®æ¤œè¨¼) ã‚’å°å…¥ã—ã€ã‚¹ã‚³ã‚¢ã®å®‰å®šæ€§ã‚’è©•ä¾¡\n",
    " - Accuracyã ã‘ã§ãªãAUCã‚¹ã‚³ã‚¢ã‚‚ç¢ºèª\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ==========================================\n",
    "# 1. å³å¯†è¨­å®š\n",
    "# ==========================================\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_SAMPLES = 2000  # ãƒ‡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…å¢—\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ==========================================\n",
    "# 2. é«˜é›£æ˜“åº¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\n",
    "# ==========================================\n",
    "def generate_rigorous_dataset(model, tokenizer, n_samples):\n",
    "    print(f\"Generating {n_samples} samples for RIGOROUS scanning...\")\n",
    "    \n",
    "    # --- Positive (çœŸã®å‰µé€ æ€§) ---\n",
    "    prompts_pos = [\n",
    "        \"Write a short poem using only simple words:\",\n",
    "        \"Describe a complex emotion using a metaphor:\",\n",
    "        \"Compose a mysterious story opening:\",\n",
    "        \"Propose a groundbreaking research idea combining quantum physics and LLMs:\",\n",
    "        \"Suggest a novel method to train AI without using any human data:\",\n",
    "    ]\n",
    "\n",
    "    # --- Negative 1: Boring (é€€å±ˆ) ---\n",
    "    prompts_boring = [\n",
    "        \"State a dry fact about history:\",\n",
    "        \"Explain a basic grammatical rule:\",\n",
    "        \"Describe the process of boiling water:\",\n",
    "    ]\n",
    "    \n",
    "    # --- Negative 2: Hard Negative (ã‚¯ãƒªã‚·ã‚§ãƒ»å°‚é–€ç”¨èªã®ç¾…åˆ—) ---\n",
    "    # ã“ã‚Œã‚‰ã¯ã€Œé›£ã—ãã†ãªå˜èªã€ã‚’å«ã‚€ãŒã€å‰µé€ çš„ã§ã¯ãªã„ï¼ˆLabel 0ï¼‰\n",
    "    # å±¤ã«ã‚ˆã£ã¦ã¯ã“ã‚Œã‚‰ã‚’ã€Œå‰µé€ çš„ã€ã¨å‹˜é•ã„ã—ã‚„ã™ã„\n",
    "    prompts_jargon = [\n",
    "        \"Write a sentence full of technical buzzwords but with no meaning:\",\n",
    "        \"Generate a generic corporate mission statement:\",\n",
    "        \"Write a standard, clichÃ© introduction for a machine learning paper:\",\n",
    "    ]\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 50% Positive\n",
    "    for _ in tqdm(range(int(n_samples * 0.5)), desc=\"Generating Positives\"):\n",
    "        prompt = random.choice(prompts_pos)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.95)\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "        data.append(text)\n",
    "        labels.append(1)\n",
    "\n",
    "    # 50% Negative (Mix of Boring and Hard Negatives)\n",
    "    for _ in tqdm(range(int(n_samples * 0.5)), desc=\"Generating Negatives\"):\n",
    "        if random.random() < 0.5:\n",
    "            # Boring\n",
    "            prompt = random.choice(prompts_boring)\n",
    "            temp = 0.1\n",
    "        else:\n",
    "            # Hard Negative (Jargon/ClichÃ©)\n",
    "            prompt = random.choice(prompts_jargon)\n",
    "            temp = 0.8 # å°‘ã—å¤šæ§˜æ€§ã‚’æŒãŸã›ã¦ç´›ã‚‰ã‚ã—ãã™ã‚‹\n",
    "            \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=temp)\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "        data.append(text)\n",
    "        labels.append(0)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# ==========================================\n",
    "# 3. ç‰¹å¾´é‡æŠ½å‡º (ãƒ¡ãƒ¢ãƒªç®¡ç†å¼·åŒ–)\n",
    "# ==========================================\n",
    "def extract_all_layers(model, tokenizer, texts, batch_size=8):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    \n",
    "    print(\"Extracting features from ALL layers...\")\n",
    "    \n",
    "    # ãƒãƒƒãƒå‡¦ç†\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i : i+batch_size]\n",
    "        batch_texts = [t if t.strip() else \"empty\" for t in batch_texts]\n",
    "        \n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        mask = inputs.attention_mask.unsqueeze(-1) # [Batch, Seq, 1]\n",
    "        \n",
    "        batch_layer_features = []\n",
    "        \n",
    "        # å…¨å±¤ãƒ«ãƒ¼ãƒ—\n",
    "        for h_state in outputs.hidden_states:\n",
    "            h_state = h_state.float() # [Batch, Seq, Dim]\n",
    "            \n",
    "            # Global Average Pooling (Masked)\n",
    "            masked_h = h_state * mask\n",
    "            sum_h = masked_h.sum(dim=1)\n",
    "            count = mask.sum(dim=1)\n",
    "            mean_h = sum_h / count.clamp(min=1e-9) # [Batch, Dim]\n",
    "            \n",
    "            batch_layer_features.append(mean_h.cpu().numpy())\n",
    "            \n",
    "        # [Layers, Batch, Dim] -> [Batch, Layers, Dim]\n",
    "        batch_layer_features = np.stack(batch_layer_features, axis=1)\n",
    "        all_features.append(batch_layer_features)\n",
    "\n",
    "    return np.concatenate(all_features, axis=0)\n",
    "\n",
    "# ==========================================\n",
    "# 4. å³å¯†è©•ä¾¡ãƒ«ãƒ¼ãƒ— (5-Fold CV)\n",
    "# ==========================================\n",
    "print(f\"Loading Model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# A. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "texts, labels = generate_rigorous_dataset(model, tokenizer, N_SAMPLES)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# B. ç‰¹å¾´é‡æŠ½å‡º [Samples, Layers, Dim]\n",
    "X_all = extract_all_layers(model, tokenizer, texts)\n",
    "num_layers = X_all.shape[1]\n",
    "\n",
    "# C. è©•ä¾¡\n",
    "results = []\n",
    "print(\"\\n=== ğŸ›‘ Running 5-Fold Cross-Validation per Layer ===\")\n",
    "print(f\"{'Layer':<5} | {'Mean Acc':<10} | {'Std Dev':<10} | {'Status'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# äº¤å·®æ¤œè¨¼ã®è¨­å®š (5åˆ†å‰², ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Š)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for layer_i in range(num_layers):\n",
    "    X_layer = X_all[:, layer_i, :]\n",
    "    \n",
    "    # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° (Linear Probe)\n",
    "    clf = LogisticRegression(max_iter=2000, solver='liblinear')\n",
    "    \n",
    "    # 5å›å­¦ç¿’ãƒ»è©•ä¾¡ã‚’è¡Œã†\n",
    "    scores = cross_val_score(clf, X_layer, labels, cv=skf, scoring='accuracy')\n",
    "    \n",
    "    mean_acc = scores.mean()\n",
    "    std_acc = scores.std()\n",
    "    \n",
    "    status = \"\"\n",
    "    if mean_acc > 0.95: status = \"â­â­ Excellent\"\n",
    "    elif mean_acc > 0.90: status = \"â­ Good\"\n",
    "    \n",
    "    results.append({\n",
    "        \"Layer\": layer_i,\n",
    "        \"Mean_Acc\": mean_acc,\n",
    "        \"Std_Acc\": std_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"{layer_i:<5} | {mean_acc:.4f}     | Â±{std_acc:.4f}   | {status}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. å¯è¦–åŒ–ãƒ»ä¿å­˜\n",
    "# ==========================================\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# ã‚°ãƒ©ãƒ•æç”» (ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ã)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(df[\"Layer\"], df[\"Mean_Acc\"], yerr=df[\"Std_Acc\"], fmt='-o', ecolor='red', capsize=5)\n",
    "plt.title(f\"Rigorous Layer Separability Analysis (5-Fold CV)\\nModel: {MODEL_NAME}\")\n",
    "plt.xlabel(\"Layer Index\")\n",
    "plt.ylabel(\"Mean Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"rigorous_layer_scan.png\")\n",
    "\n",
    "# ãƒ™ã‚¹ãƒˆãƒ¬ã‚¤ãƒ¤ãƒ¼é¸å®š (å¹³å‡ç²¾åº¦ãŒé«˜ãã€ã‹ã¤å®‰å®šã—ã¦ã„ã‚‹ã‚‚ã®)\n",
    "# ã‚½ãƒ¼ãƒˆ: å¹³å‡ç²¾åº¦é™é †\n",
    "best_row = df.sort_values(by=\"Mean_Acc\", ascending=False).iloc[0]\n",
    "best_layer = int(best_row[\"Layer\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"ğŸ† FINAL VERDICT (Strict Mode)\")\n",
    "print(f\"Best Layer: {best_layer}\")\n",
    "print(f\"Accuracy  : {best_row['Mean_Acc']:.4f} (Â±{best_row['Std_Acc']:.4f})\")\n",
    "print(\"=\"*50)\n",
    "print(\"Please update 'TARGET_LAYER' in your probe script to this value.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

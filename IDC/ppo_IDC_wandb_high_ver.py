#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ppo_transformer_probe_hybrid.py

Hybrid Reward PPO:
 - Policy: Qwen/Qwen2.5-7B-Instruct (with LoRA)
 - Extrinsic Reward: DeBERTa (Quality/Safety)
 - Intrinsic Reward: Transformer Probe (Creativity/Context-Aware)
 - Strategy: Gated Hybrid (Quality Check -> Creativity Bonus)

Usage:
python ppo_transformer_probe_hybrid.py \
    --probe-path "transformer_creativity_probe.pt" \
    --wandb-run-name "run-creative-probe-v1" \
    --w-ext 1.0 \
    --w-int 0.3 \
    --gate-threshold -1.5


python ppo_IDC_wandb_high_ver.py \
    --probe-path "transformer_creativity_probe_enhanced.pt" \
    --wandb-run-name "run-research-enhanced-v1" \
    --num-steps 500 \
    --w-ext 1.0 \
    --w-int 0.3 \
    --gate-threshold -0.5 \
    --batch-size 32 \
    --mini-batch-size 8 \
    --lr 1.41e-5 \
    --init-kl-coef 0.05
"""

import argparse
import random
import textwrap
import numpy as np
import torch
import torch.nn as nn
import wandb
import os
import time
from tqdm import tqdm
import datasets 

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
from trl import (
    PPOConfig,
    PPOTrainer,
    AutoModelForCausalLMWithValueHead,
    create_reference_model,
)
from peft import LoraConfig

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§ãƒ‡ãƒã‚¤ã‚¹ã‚’å®šç¾©
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ============================================================
# 1. Probe Model Architecture (Must match saved model)
# ============================================================
class TransformerProbe(nn.Module):
    def __init__(self, input_dim, d_model=256, nhead=4):
        super().__init__()
        self.project = nn.Linear(input_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=512, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)
        self.head = nn.Linear(d_model, 1)
        
    def forward(self, x):
        # x: [Batch, Seq, Input_Dim]
        x = self.project(x)
        x = self.transformer(x)
        # Global Average Pooling
        x = x.mean(dim=1) 
        return self.head(x)

class TransformerProbeRewardModel(nn.Module):
    """
    ä¿å­˜ã•ã‚ŒãŸProbeã‚’èª­ã¿è¾¼ã¿ã€å ±é…¬ï¼ˆLogitï¼‰ã‚’è¨ˆç®—ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼
    """
    def __init__(self, path: str, device: torch.device):
        super().__init__()
        self.device = device
        
        try:
            checkpoint = torch.load(path, map_location=device)
            config = checkpoint["config"]
            
            self.layer_idx = config["layer_idx"]
            self.model = TransformerProbe(
                input_dim=config["input_dim"],
                d_model=config["d_model"],
                nhead=config["nhead"]
            ).to(device)
            
            self.model.load_state_dict(checkpoint["model_state_dict"])
            self.model.eval()
            
            # PPOä¸­ã«ProbeãŒæ›´æ–°ã•ã‚Œãªã„ã‚ˆã†ã«å‡çµ
            for param in self.model.parameters():
                param.requires_grad = False
                
            print(f"[INFO] Probe Loaded: Layer {self.layer_idx}, Dim {config['input_dim']}")
            
        except Exception as e:
            print(f"[ERROR] Failed to load probe '{path}': {e}")
            raise e

    def get_reward(self, hidden_states: torch.Tensor) -> float:
        """
        hidden_states: [Seq, Dim] (1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹)
        """
        with torch.no_grad():
            # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ  [1, Seq, Dim]
            x = hidden_states.unsqueeze(0).to(self.device)
            logit = self.model(x).item()
        return logit

# ============================================================
# 2. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def shorten(text: str, width: int = 256) -> str:
    text = text.replace("\n", " ")
    return textwrap.shorten(text, width=width, placeholder="...")

# ============================================================
# 3. å ±é…¬è¨ˆç®—é–¢æ•°
# ============================================================
@torch.no_grad()
def compute_external_rm_rewards(
    rm_model, rm_tokenizer,
    problems, responses,
    device,
    max_length=512
):
    """å¤–éƒ¨ RM (DeBERTa) ã®å ±é…¬ã‚’è¨ˆç®— (Quality)"""
    inputs = []
    for p, r in zip(problems, responses):
        # Chatå½¢å¼ã‚„Instructå½¢å¼ã«åˆã‚ã›ã¦èª¿æ•´
        txt = f"User: {shorten(p)}\nAssistant: {r}"
        inputs.append(txt)
        
    enc = rm_tokenizer(inputs, return_tensors="pt", padding=True, truncation=True, max_length=max_length).to(device)
    out = rm_model(**enc)
    
    # Logits (é€šå¸¸ã¯ [Batch, 1] ã¾ãŸã¯ [Batch, 2])
    if out.logits.shape[-1] == 1:
        return out.logits.squeeze(-1).tolist()
    else:
        return out.logits[:, 0].tolist()

@torch.no_grad()
def compute_internal_probe_rewards(
    policy_model, tokenizer,
    prompts, response_tensors,
    probe_wrapper
):
    """
    ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®HiddenStatesã‚’å–å¾—ã—ã€Probeã§å‰µé€ æ€§ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã™ã‚‹
    """
    rewards = []
    device = policy_model.pretrained_model.device
    
    # Tensor -> Text
    responses_text = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)
    
    for p, r_text in zip(prompts, responses_text):
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ+å¿œç­”ã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã¦Hidden Stateã‚’å–å¾—
        full_text = p + r_text
        inp = tokenizer(full_text, return_tensors="pt").to(device)
        
        # Forward Pass (Gradientä¸è¦)
        out = policy_model.pretrained_model(**inp, output_hidden_states=True)
        
        # æŒ‡å®šå±¤ã®Hidden Stateã‚’å–å¾—
        target_layer = probe_wrapper.layer_idx
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã‚’å–å¾— (ProbeãŒTransformerãªã®ã§æ–‡è„ˆãŒå¿…è¦)
        # [1, Seq, Dim] -> [Seq, Dim]
        h_seq = out.hidden_states[target_layer].squeeze(0).float()
        
        # Probeæ¨è«–
        score = probe_wrapper.get_reward(h_seq)
        rewards.append(score)
        
    return rewards

# ============================================================
# 4. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================
def main():
    parser = argparse.ArgumentParser()

    # Models
    parser.add_argument("--policy-model-name", type=str, default="Qwen/Qwen2.5-7B-Instruct")
    parser.add_argument("--rm-model-name", type=str, default="OpenAssistant/reward-model-deberta-v3-large-v2")
    parser.add_argument("--probe-path", type=str, default="transformer_creativity_probe.pt", help="Path to saved probe")

    # PPO Params
    parser.add_argument("--num-steps", type=int, default=200, help="Total optimization steps")
    parser.add_argument("--batch-size", type=int, default=16)
    parser.add_argument("--mini-batch-size", type=int, default=4)
    parser.add_argument("--max-new-tokens", type=int, default=128)
    parser.add_argument("--lr", type=float, default=1.41e-5)
    parser.add_argument("--init-kl-coef", type=float, default=0.1)
    parser.add_argument("--seed", type=int, default=42)

    # Hybrid Reward Params
    parser.add_argument("--w-ext", type=float, default=1.0, help="å¤–éƒ¨å ±é…¬(å“è³ª)ã®é‡ã¿")
    parser.add_argument("--w-int", type=float, default=0.3, help="å†…éƒ¨å ±é…¬(å‰µé€ æ€§)ã®é‡ã¿")
    parser.add_argument("--gate-threshold", type=float, default=-1.5, help="è¶³åˆ‡ã‚Šãƒ©ã‚¤ãƒ³ã€‚ã“ã‚Œä»¥ä¸‹ã®å“è³ªãªã‚‰å‰µé€ æ€§ãƒœãƒ¼ãƒŠã‚¹ç„¡åŠ¹")

    # W&B
    parser.add_argument("--wandb-project", type=str, default="qwen-creative-ppo")
    parser.add_argument("--wandb-run-name", type=str, default=None)

    args = parser.parse_args()
    set_seed(args.seed)

    # --------------------------------------------------------
    # PPO Configuration
    # --------------------------------------------------------
    ppo_config = PPOConfig(
        model_name=args.policy_model_name,
        learning_rate=args.lr,
        batch_size=args.batch_size,
        mini_batch_size=args.mini_batch_size,
        gradient_accumulation_steps=1,
        optimize_cuda_cache=True,
        target_kl=0.1,
        init_kl_coef=args.init_kl_coef,
        ppo_epochs=4,
        seed=args.seed,
        log_with="wandb",
        tracker_project_name=args.wandb_project,
        tracker_kwargs={"wandb": {"name": args.wandb_run_name}}
    )

    # --------------------------------------------------------
    # Model Loading (with LoRA)
    # --------------------------------------------------------
    print(f"[INFO] Loading Policy Model: {args.policy_model_name}")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ã¨è¨­å®š (ã“ã“ã‚’ä¿®æ­£)
    tokenizer = AutoTokenizer.from_pretrained(ppo_config.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token 
    tokenizer.padding_side = "left" # å­¦ç¿’æ™‚ã®é‡è¦è¨­å®š

    # LoRA Config
    lora_config = LoraConfig(
        r=16, 
        lora_alpha=32, 
        lora_dropout=0.05, 
        bias="none", 
        task_type="CAUSAL_LM"
    )

    # Policy Model
    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        ppo_config.model_name,
        torch_dtype=torch.float16,
        peft_config=lora_config,
        device_map="auto"
    )
    
    # Reference Model
    ref_model = create_reference_model(model)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    base_inst = (
        "You are an expert researcher. Propose a novel and concrete research idea.\n"
        "Output ONLY in the following format:\n\n"
        "Title: <concise title>\n"
        "Abstract: <150-200 word abstract>\n"
    )
    topics = [
        "Mixture of Experts", "State Space Models", "Sparse Attention",
        "KV-Cache Optimization", "Continual Learning", "Federated Learning",
        "Quantization", "Knowledge Distillation", "LoRA", "RLHF", "DPO", 
        "Chain of Thought", "Multi-Agent", "Tool use", "RAG", 
        "Synthetic data", "Multilingual"
    ]
    perspectives = ["efficiency", "interpretability", "robustness", "reasoning", "cost"]
    
    prompt_list = []
    for t in topics:
        for p in perspectives:
            txt = f"Draft a research proposal about {t} focusing on {p}.\n{base_inst}"
            prompt_list.append(txt)
    
    print(f"[INFO] Generated {len(prompt_list)} prompts.")
    dataset = datasets.Dataset.from_dict({"query": prompt_list})
    
    def collator(data):
        return {key: [d[key] for d in data] for key in data[0]}

    trainer = PPOTrainer(
        config=ppo_config,
        model=model,
        ref_model=ref_model,
        tokenizer=tokenizer,
        dataset=dataset,
        data_collator=collator
    )
    
    # ãƒ‡ãƒã‚¤ã‚¹ã®å–å¾— (accelerateçµŒç”±ãŒå®‰å…¨)
    device = trainer.accelerator.device

    # --------------------------------------------------------
    # Reward Models Loading
    # --------------------------------------------------------
    print(f"[INFO] Loading Extrinsic RM: {args.rm_model_name}")
    rm_tokenizer = AutoTokenizer.from_pretrained(args.rm_model_name)
    rm_model = AutoModelForSequenceClassification.from_pretrained(
        args.rm_model_name, torch_dtype=torch.float16
    ).to(device)
    rm_model.eval()

    print(f"[INFO] Loading Intrinsic Probe: {args.probe_path}")
    probe_wrapper = TransformerProbeRewardModel(args.probe_path, device)

    # --------------------------------------------------------
    # Training Loop
    # --------------------------------------------------------
    print(f"[INFO] Starting Training for {args.num_steps} steps...")
    
    # DataLoaderã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿
    data_iter = iter(trainer.dataloader)

    for step in tqdm(range(args.num_steps)):
        # ãƒ‡ãƒ¼ã‚¿ã®å–å¾— (å®‰å…¨ç­–ã‚’è¿½åŠ )
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(trainer.dataloader)
            batch = next(data_iter)
        
        # å®‰å…¨ç­–: ãƒãƒƒãƒãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ— (IndexErrorå›é¿)
        if not batch or "query" not in batch or len(batch["query"]) == 0:
            print("[WARNING] Empty batch detected. Skipping step.")
            continue

        query_tensors = []
        response_tensors = []
        
        # --- 1. Generation (Rollout) ---
        for query in batch["query"]:
            # attention_maskã‚’å–å¾—
            inputs = tokenizer(query, return_tensors="pt").to(device)
            query_tensor = inputs.input_ids.squeeze(0)
            attention_mask = inputs.attention_mask
            
            query_tensors.append(query_tensor)
            
            generation_kwargs = {
                "min_length": -1,
                "top_k": 0.0,
                "top_p": 1.0,
                "do_sample": True,
                "pad_token_id": tokenizer.eos_token_id,
                "max_new_tokens": args.max_new_tokens,
                "temperature": 0.95,
                "attention_mask": attention_mask, # è­¦å‘Šå¯¾ç­–
            }
            
            response = trainer.generate(query_tensor, **generation_kwargs).squeeze(0)
            response_tensors.append(response[len(query_tensor):])

        # å®‰å…¨ç­–: ä½•ã‚‰ã‹ã®ç†ç”±ã§ãƒªã‚¹ãƒˆãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if len(query_tensors) == 0:
            continue

        # --- 2. Reward Calculation ---
        batch_responses = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)
        
        # A. Extrinsic (Quality)
        ext_scores = compute_external_rm_rewards(
            rm_model, rm_tokenizer, batch["query"], batch_responses, device
        )
        
        # B. Intrinsic (Creativity Probe)
        int_scores = compute_internal_probe_rewards(
            trainer.model, tokenizer, batch["query"], response_tensors, probe_wrapper
        )

        # C. Hybrid & Gating
        final_rewards = []
        stats_ext = []
        stats_int = []
        gated_count = 0
        texts_for_log = []

        for q, r, ext, int_val in zip(batch["query"], batch_responses, ext_scores, int_scores):
            if ext < args.gate_threshold:
                total = float(ext)
                gated_count += 1
            else:
                total = float(ext) + (args.w_int * float(int_val))
            
            final_rewards.append(torch.tensor(total, device=device))
            
            stats_ext.append(ext)
            stats_int.append(int_val)
            texts_for_log.append([q, r, total, ext, int_val])

        # --- 3. PPO Update ---
        # ã“ã“ã§ IndexError ãŒå‡ºãªã„ã‚ˆã†ã€ç©ºãƒªã‚¹ãƒˆã§ãªã„ã“ã¨ã‚’å‰æã¨ã™ã‚‹
        stats = trainer.step(query_tensors, response_tensors, final_rewards)

        # --- 4. Logging ---
        stats["env/reward_mean_total"] = np.mean([r.item() for r in final_rewards])
        stats["env/reward_mean_ext"] = np.mean(stats_ext)
        stats["env/reward_mean_int"] = np.mean(stats_int)
        stats["env/gated_ratio"] = gated_count / len(batch["query"])
        
        if step % 10 == 0 and wandb.run is not None:
            table = wandb.Table(
                columns=["Query", "Response", "Total", "Ext(Qual)", "Int(Creat)"],
                data=texts_for_log
            )
            wandb.log({"game_log": table}, step=step)

        # â˜…ä¿®æ­£: batchã«responseã‚’è¿½åŠ  (trlã®ãƒ­ã‚°è¦ä»¶)
        batch["response"] = batch_responses
        
        trainer.log_stats(stats, batch, final_rewards)
        
        if step % 5 == 0:
            print(f"\n[Step {step}] Total: {stats['env/reward_mean_total']:.2f} | "
                  f"Ext: {np.mean(stats_ext):.2f} | Int: {np.mean(stats_int):.2f}")

    # --------------------------------------------------------
    # Save Model
    # --------------------------------------------------------
    save_dir = f"./saved_models/{args.wandb_run_name}" if args.wandb_run_name else "./saved_models/ppo_final"
    
    # ã€ä¿®æ­£ç®‡æ‰€ã€‘ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆã™ã‚‹
    if not os.path.exists(save_dir):
        os.makedirs(save_dir, exist_ok=True)
        print(f"[INFO] Created directory: {save_dir}")

    print(f"\n[INFO] Saving model to {save_dir} ...")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜
    trainer.model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    
    wandb.finish()
    
    print("ğŸ”” å­¦ç¿’ãŒçµ‚äº†ã—ã¾ã—ãŸï¼")
    for _ in range(3):
        print('\a')
        time.sleep(0.5)

if __name__ == "__main__":
    main()
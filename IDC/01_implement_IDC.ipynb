{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee42b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 1. è¨­å®š (æ—¢å­˜ç’°å¢ƒç¶™æ‰¿)\n",
    "SUBSPACE_PATH = \"common_subspace.pt\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ğŸ§  Internal Divergence Curiosity (IDC) ãƒ¢ãƒ‡ãƒ«å®šç¾©\n",
    "# -----------------------------------------------------------\n",
    "class IDCRewardModel:\n",
    "    def __init__(self, basis, mean, device, memory_capacity=2048):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            basis: PCAåŸºåº• (å…±é€šæˆåˆ†)\n",
    "            mean: PCAå¹³å‡\n",
    "            memory_capacity: è¨˜æ†¶ã™ã‚‹éå»ã®ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ™ã‚¯ãƒˆãƒ«ã®æœ€å¤§æ•°\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.basis = basis.to(device)\n",
    "        self.mean = mean.to(device)\n",
    "        \n",
    "        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ (FIFOãƒãƒƒãƒ•ã‚¡)\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.memory_buffer = torch.empty(0, basis.shape[1], device=device, dtype=basis.dtype)\n",
    "        \n",
    "    def get_semantic_vector(self, h):\n",
    "        \"\"\"\n",
    "        éš ã‚ŒçŠ¶æ…‹ h ã‹ã‚‰ã€Œæ„å‘³è«–ãƒ™ã‚¯ãƒˆãƒ«(æ®‹å·®)ã€ã‚’æŠ½å‡ºã™ã‚‹\n",
    "        Common Subspace (å¹³å‡¡ãªæˆåˆ†) ã‚’é™¤å»ã—ãŸã‚‚ã®ã‚’ä½¿ç”¨\n",
    "        \"\"\"\n",
    "        h = h.to(self.basis.dtype)\n",
    "        h_centered = h - self.mean\n",
    "        \n",
    "        # å…±é€šæˆåˆ†ã®ç®—å‡ºã¨é™¤å»\n",
    "        # h_common = (h_centered @ basis.T) @ basis\n",
    "        z_common = h_centered @ self.basis.T\n",
    "        h_common = z_common @ self.basis\n",
    "        \n",
    "        # æ®‹å·®ãƒ™ã‚¯ãƒˆãƒ« = æ„å‘³è«–ãƒ™ã‚¯ãƒˆãƒ« (Semantic Vector)\n",
    "        h_semantic = h_centered - h_common\n",
    "        return h_semantic\n",
    "\n",
    "    def compute_reward_and_update(self, h_batch):\n",
    "        \"\"\"\n",
    "        ãƒãƒƒãƒå†…ã®å„éš ã‚ŒçŠ¶æ…‹ã«ã¤ã„ã¦ã€è¨˜æ†¶ã¨ã®è·é›¢(å ±é…¬)ã‚’è¨ˆç®—ã—ã€è¨˜æ†¶ã‚’æ›´æ–°ã™ã‚‹\n",
    "        Args:\n",
    "            h_batch: (Batch, Dim) or (Seq, Dim) ã®éš ã‚ŒçŠ¶æ…‹\n",
    "        Returns:\n",
    "            rewards: (Batch,) å„ã‚¹ãƒ†ãƒƒãƒ—ã®æ‹¡æ•£çš„å¥½å¥‡å¿ƒå ±é…¬\n",
    "        \"\"\"\n",
    "        # 1. æ„å‘³è«–ãƒ™ã‚¯ãƒˆãƒ«ã®æŠ½å‡º\n",
    "        current_vectors = self.get_semantic_vector(h_batch)\n",
    "        \n",
    "        # 2. æ­£è¦åŒ– (ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ã®ãŸã‚)\n",
    "        current_vectors_norm = F.normalize(current_vectors, p=2, dim=1)\n",
    "        \n",
    "        rewards = []\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªãŒç©ºã®å ´åˆã®å‡¦ç†\n",
    "        if self.memory_buffer.shape[0] == 0:\n",
    "            # æœ€åˆã¯æ¯”è¼ƒå¯¾è±¡ãŒãªã„ã®ã§å ±é…¬ã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³(ä¾‹: 0.5)ã‚„æœ€å¤§å€¤ã‚’ä¸ãˆã‚‹\n",
    "            # ã“ã“ã§ã¯ã€Œéå»ãŒãªã„=æœ€å¤§ã®é©šãã€ã¨ã—ã¦ 1.0 ã‚’ä¸ãˆã‚‹\n",
    "            rewards = torch.ones(current_vectors.shape[0], device=self.device)\n",
    "        else:\n",
    "            # 3. è¨˜æ†¶ã¨ã®é¡ä¼¼åº¦è¨ˆç®— (Matrix Multiplication)\n",
    "            # Memoryã¯æ—¢ã«æ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹ã¨ä»®å®šã—ã¦è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›ã‚‚å¯èƒ½ã ãŒã€\n",
    "            # ã“ã“ã§ã¯å®‰å…¨ã®ãŸã‚éƒ½åº¦æ­£è¦åŒ–ã™ã‚‹å®Ÿè£…ã¨ã™ã‚‹\n",
    "            memory_norm = F.normalize(self.memory_buffer, p=2, dim=1)\n",
    "            \n",
    "            # Cosine Similarity: [Batch, Memory_Size]\n",
    "            sim_matrix = torch.mm(current_vectors_norm, memory_norm.T)\n",
    "            \n",
    "            # 4. ã€Œæœ€ã‚‚ä¼¼ã¦ã„ã‚‹éå»ã€ã‚’æ¢ã™ (Max Similarity)\n",
    "            # æ—¢è¦–æ„Ÿ = Max Sim.  æ‹¡æ•£çš„å¥½å¥‡å¿ƒ = 1 - æ—¢è¦–æ„Ÿ\n",
    "            max_sim, _ = torch.max(sim_matrix, dim=1)\n",
    "            \n",
    "            # å ±é…¬è¨ˆç®— (è·é›¢)\n",
    "            # Simã¯ -1~1 ãªã®ã§ã€ 1 - Sim ã¯ 0~2 ã®ç¯„å›²ã«ãªã‚‹\n",
    "            rewards = 1.0 - max_sim\n",
    "            \n",
    "        # 5. è¨˜æ†¶ã®æ›´æ–° (ç¾åœ¨ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨˜æ†¶ã«è¿½åŠ )\n",
    "        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãŸã‚ã«ã€ä»Šã®ã€Œæ–°ã—ã„ä½“é¨“ã€ã‚’è¨˜æ†¶ã™ã‚‹\n",
    "        self.memory_buffer = torch.cat([self.memory_buffer, current_vectors.detach()], dim=0)\n",
    "        \n",
    "        # å®¹é‡ã‚ªãƒ¼ãƒãƒ¼ã—ãŸã‚‰å¤ã„è¨˜æ†¶ã‚’æ¨ã¦ã‚‹ (FIFO)\n",
    "        if self.memory_buffer.shape[0] > self.memory_capacity:\n",
    "            self.memory_buffer = self.memory_buffer[-self.memory_capacity:]\n",
    "            \n",
    "        return rewards\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã®ãƒªã‚»ãƒƒãƒˆç”¨\"\"\"\n",
    "        self.memory_buffer = torch.empty(0, self.basis.shape[1], device=self.device, dtype=self.basis.dtype)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ğŸ§ª æ¤œè¨¼ã‚³ãƒ¼ãƒ‰: å¹³å‡¡ãªç¹°ã‚Šè¿”ã— vs å‰µé€ çš„ãªå±•é–‹\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# 1. ãƒ¢ãƒ‡ãƒ«ã¨ã‚µãƒ–ã‚¹ãƒšãƒ¼ã‚¹ã®ãƒ­ãƒ¼ãƒ‰ (å‰ã®ã‚»ãƒ«ã§ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰çœç•¥å¯)\n",
    "try:\n",
    "    # æ—¢å­˜ã® basis, mean ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†\n",
    "    idc_model = IDCRewardModel(basis, mean, DEVICE, memory_capacity=512)\n",
    "    print(\"IDC Model Initialized with existing Subspace.\")\n",
    "except NameError:\n",
    "    print(\"Loading Subspace...\")\n",
    "    data = torch.load(SUBSPACE_PATH, map_location=DEVICE, weights_only=False)\n",
    "    basis = data[\"basis\"].to(dtype=torch.float16)\n",
    "    mean = data[\"mean\"].to(dtype=torch.float16)\n",
    "    idc_model = IDCRewardModel(basis, mean, DEVICE, memory_capacity=512)\n",
    "\n",
    "# 2. ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "# Prompt A: åŒã˜ãƒˆãƒ”ãƒƒã‚¯ã‚’ç¹°ã‚Šè¿”ã™ (å ±é…¬ãŒä¸‹ãŒã‚‹ã¯ãš)\n",
    "# Prompt B: å¸¸ã«æ–°ã—ã„å±•é–‹ã‚’ã™ã‚‹ (å ±é…¬ãŒç¶­æŒã•ã‚Œã‚‹ã¯ãš)\n",
    "prompts = [\n",
    "    {\n",
    "        \"type\": \"Repetitive (DejÃ  Vu)\",\n",
    "        \"text\": \"The cat sat on the mat. The cat sat on the mat. The cat sat on the mat. The cat sat on the mat.\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"Diverse (Diffusive)\",\n",
    "        \"text\": \"The cat sat on the mat. Suddenly, a spaceship landed in the garden. Aliens came out and started dancing tango.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3. å®Ÿè¡Œã¨å¯è¦–åŒ–\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for p_data in prompts:\n",
    "    label = p_data[\"type\"]\n",
    "    text = p_data[\"text\"]\n",
    "    \n",
    "    # è¨˜æ†¶ã‚’ãƒªã‚»ãƒƒãƒˆ (ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹)\n",
    "    idc_model.clear_memory()\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # éš ã‚ŒçŠ¶æ…‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ [Seq_Len, Dim]\n",
    "    # ãƒãƒƒãƒã‚µã‚¤ã‚º1å‰æ\n",
    "    h_seq = outputs.hidden_states[-1].squeeze(0)\n",
    "    \n",
    "    # é€æ¬¡çš„ã«å ±é…¬ã‚’è¨ˆç®— (æ™‚ç³»åˆ—å¤‰åŒ–ã‚’è¦‹ã‚‹ãŸã‚)\n",
    "    step_rewards = []\n",
    "    \n",
    "    # æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰é †ç•ªã«å‡¦ç†\n",
    "    # (å®Ÿéš›ã¯ã¾ã¨ã‚ã¦è¨ˆç®—ã§ãã¾ã™ãŒã€è¨˜æ†¶ã®æ›´æ–°éç¨‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ãŸã‚ãƒ«ãƒ¼ãƒ—)\n",
    "    for t in range(h_seq.shape[0]):\n",
    "        h_t = h_seq[t].unsqueeze(0) # [1, Dim]\n",
    "        r_t = idc_model.compute_reward_and_update(h_t)\n",
    "        step_rewards.append(r_t.item())\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    tokens = [tokenizer.decode(t) for t in inputs.input_ids[0]]\n",
    "    plt.plot(step_rewards, marker='o', label=label)\n",
    "    \n",
    "    # å¹³å‡ã‚¹ã‚³ã‚¢è¡¨ç¤º\n",
    "    avg_score = sum(step_rewards) / len(step_rewards)\n",
    "    print(f\"Type: {label:20s} | Avg IDC Reward: {avg_score:.4f}\")\n",
    "\n",
    "plt.title(\"IDC Reward Transition: Repetitive vs Diverse\")\n",
    "plt.xlabel(\"Token Step\")\n",
    "plt.ylabel(\"IDC Reward (1 - Max Similarity)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

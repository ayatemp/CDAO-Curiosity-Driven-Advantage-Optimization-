{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad64f3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b306b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ==========================================\n",
    "# 1. è¨­å®š & å®šæ•°\n",
    "# ==========================================\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "SAVE_PATH = \"transformer_creativity_probe_enhanced.pt\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TARGET_LAYER = 24  # ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹å±¤ (Qwen-7Bã®ä¸­æ·±å±¤)\n",
    "Input_Dim = 3584   # Qwen-2.5-7B ã®éš ã‚Œå±¤æ¬¡å…ƒæ•°\n",
    "\n",
    "# å†ç¾æ€§ç¢ºä¿\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d7a4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. ãƒ¢ãƒ‡ãƒ«å®šç¾© (Probe)\n",
    "# ==========================================\n",
    "class TransformerProbe(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=256, nhead=4):\n",
    "        super().__init__()\n",
    "        self.project = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=512, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, Input_Dim]\n",
    "        x = self.project(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1) # Global Average Pooling\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f543ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ (Enhanced)\n",
    "# ==========================================\n",
    "def generate_enhanced_dataset(model, tokenizer, n_samples=3000):\n",
    "    model.eval()\n",
    "    print(f\"Generating {n_samples} samples with Research-Aware strategies...\")\n",
    "\n",
    "    # --- A. Positive Prompts (Label = 1.0) ---\n",
    "    # æ–¬æ–°ãªç™ºæƒ³ã€ç•°åˆ†é‡èåˆã€è©©çš„ãªè¡¨ç¾\n",
    "    prompts_pos = [\n",
    "        \"Write a short poem using only simple words:\",\n",
    "        \"Describe a complex emotion using a metaphor:\",\n",
    "        \"Compose a mysterious story opening:\",\n",
    "        # â˜… ç ”ç©¶: ç•°åˆ†é‡èåˆãƒ»æ–°è¦æ€§\n",
    "        \"Propose a groundbreaking research idea combining quantum physics and LLMs:\",\n",
    "        \"Suggest a novel method to train AI without using any human data:\",\n",
    "        \"Invent a new neural network architecture inspired by biological ecosystems:\",\n",
    "        \"Write an abstract for a paper that challenges the scaling laws of transformers:\",\n",
    "    ]\n",
    "\n",
    "    # --- B. Negative Prompts (Label = 0.0) ---\n",
    "    # é€€å±ˆã€å®šå‹æ–‡ã€ã‚ã‚ŠããŸã‚Šãªç ”ç©¶ã€å°‚é–€ç”¨èªã‚µãƒ©ãƒ€\n",
    "    prompts_boring = [\n",
    "        \"State a dry fact about history:\",\n",
    "        \"Explain a basic grammatical rule:\",\n",
    "        \"Describe the process of boiling water:\",\n",
    "    ]\n",
    "    prompts_instruction = [\n",
    "        \"Give a polite response declining a request:\",\n",
    "        \"Say 'Here is the summary' in a formal way:\",\n",
    "        \"Write a standard AI assistant response:\",\n",
    "    ]\n",
    "    # â˜… ç ”ç©¶: ã‚ã‚ŠããŸã‚Š (ClichÃ©)\n",
    "    prompts_cliche_research = [\n",
    "        \"Describe a standard MNIST digit classification experiment:\",\n",
    "        \"Explain how to fine-tune BERT for sentiment analysis:\",\n",
    "        \"Write a generic introduction about the importance of big data:\",\n",
    "        \"State the definition of supervised learning from a textbook:\",\n",
    "    ]\n",
    "    \n",
    "    # â˜… ç ”ç©¶: å°‚é–€ç”¨èªã‚µãƒ©ãƒ€ (Fake Jargon) ç”¨ã®å˜èªãƒªã‚¹ãƒˆ\n",
    "    jargon_prefixes = [\"Hyper-\", \"Quantum\", \"Neuro-\", \"Multi-\", \"Meta-\", \"Cyber-\", \"Deep\"]\n",
    "    jargon_nouns = [\"Entropy\", \"Optimization\", \"Tensors\", \"Gradients\", \"Blockchain\", \"Synergy\", \"Paradigm\"]\n",
    "    jargon_verbs = [\"disrupts\", \"leverages\", \"synthesizes\", \"encrypts\", \"validates\"]\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # ç”Ÿæˆãƒ«ãƒ¼ãƒ—\n",
    "    # Positive (40%)\n",
    "    for _ in tqdm(range(int(n_samples * 0.4)), desc=\"Positive\"):\n",
    "        prompt = random.choice(prompts_pos)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs, max_new_tokens=64, temperature=0.95, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "        data.append(text)\n",
    "        labels.append(1.0)\n",
    "\n",
    "    # Negative (60%)\n",
    "    for _ in tqdm(range(int(n_samples * 0.6)), desc=\"Negative\"):\n",
    "        r = random.random()\n",
    "        \n",
    "        if r < 0.25: # Boring\n",
    "            prompt = random.choice(prompts_boring)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**inputs, max_new_tokens=64, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "            text = tokenizer.decode(out[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "            \n",
    "        elif r < 0.50: # Instruction\n",
    "            prompt = random.choice(prompts_instruction)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**inputs, max_new_tokens=64, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "            text = tokenizer.decode(out[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "\n",
    "        elif r < 0.75: # ClichÃ© Research (ã‚ã‚ŠããŸã‚Šãªç ”ç©¶)\n",
    "            prompt = random.choice(prompts_cliche_research)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**inputs, max_new_tokens=64, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "            text = tokenizer.decode(out[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "            \n",
    "        else: # Fake Jargon (å°‚é–€ç”¨èªã‚µãƒ©ãƒ€) - ç”Ÿæˆã§ã¯ãªãåˆæˆã§ä½œã‚‹\n",
    "            p1 = random.choice(jargon_prefixes) + random.choice(jargon_nouns)\n",
    "            p2 = random.choice(jargon_prefixes) + random.choice(jargon_nouns)\n",
    "            v = random.choice(jargon_verbs)\n",
    "            text = f\"The {p1} {v} the {p2} for optimal performance within the stochastic framework.\"\n",
    "            \n",
    "        data.append(text)\n",
    "        labels.append(0.0)\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4582719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. ç‰¹å¾´é‡æŠ½å‡ºãƒ˜ãƒ«ãƒ‘ãƒ¼\n",
    "# ==========================================\n",
    "def extract_features(model, tokenizer, texts, batch_size=16):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    print(\"Extracting Hidden States...\")\n",
    "    # ãƒãƒƒãƒå‡¦ç†ã§é«˜é€ŸåŒ–\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i : i+batch_size]\n",
    "        # ç©ºæ–‡å­—å¯¾ç­–\n",
    "        batch_texts = [t if t.strip() else \"empty\" for t in batch_texts]\n",
    "        \n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        # æŒ‡å®šå±¤ã®ç‰¹å¾´é‡ã‚’å–å¾— [Batch, Seq, Dim]\n",
    "        h = outputs.hidden_states[TARGET_LAYER].float().cpu()\n",
    "        \n",
    "        for j in range(len(batch_texts)):\n",
    "            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†(mask=0)ã‚’é™¤å¤–ã™ã‚‹ã®ãŒç†æƒ³ã ãŒã€ä»Šå›ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ãã®ã¾ã¾ä½¿ç”¨\n",
    "            # (ProbeãŒTransformerãªã®ã§Attentionã§ç„¡è¦–ã™ã‚‹ã“ã¨ã‚’æœŸå¾…ã€ã‚ã‚‹ã„ã¯Truncationã§å¯¾å‡¦)\n",
    "            features.append(h[j]) \n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d3fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. å³å¯†è©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\n",
    "# ==========================================\n",
    "def strict_benchmark(probe, base_model, tokenizer):\n",
    "    print(\"\\n=== ğŸ›‘ Running Strict Research Benchmark ===\")\n",
    "    \n",
    "    cases = [\n",
    "        # --- Research Domain (æœ€é‡è¦) ---\n",
    "        (\"Research\", \"ClichÃ©\", \"Using deep learning to classify images of cats and dogs using CNNs.\"),\n",
    "        (\"Research\", \"Novel\", \"Training LLMs using simulated debates between historical figures to improve reasoning.\"),\n",
    "        (\"Research\", \"Nonsense\", \"Fine-tuning a neural network on the taste of the color blue using quantum gradients.\"),\n",
    "        \n",
    "        # --- Story Domain (ç¢ºèªç”¨) ---\n",
    "        (\"Story\", \"ClichÃ©\", \"A detective solves a murder case in a small town.\"),\n",
    "        (\"Story\", \"Novel\", \"A murder mystery where the detective is the ghost of the victim.\"),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    probe.eval()\n",
    "    \n",
    "    print(f\"{'Domain':10s} | {'Type':10s} | {'Logit':8s} | Text\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for domain, type_, text in cases:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = base_model(**inputs, output_hidden_states=True)\n",
    "            h = out.hidden_states[TARGET_LAYER].squeeze(0).float() # [Seq, Dim]\n",
    "            \n",
    "            # Probe\n",
    "            logit = probe(h.unsqueeze(0).to(DEVICE)).item()\n",
    "            \n",
    "        results.append({\"Domain\": domain, \"Type\": type_, \"Score\": logit})\n",
    "        print(f\"{domain:10s} | {type_:10s} | {logit:8.4f} | {text[:40]}...\")\n",
    "        \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2058b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941b51f3bd3c4d18a69ff76a690419d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3000 samples with Research-Aware strategies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positive: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [25:47<00:00,  1.29s/it]\n",
      "Negative: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1800/1800 [28:21<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Hidden States...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [00:32<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Probe Training...\n",
      "Epoch 0: Loss = 0.1410\n",
      "Epoch 5: Loss = 0.0009\n",
      "Epoch 10: Loss = 0.0005\n",
      "\n",
      "=== ğŸ›‘ Running Strict Research Benchmark ===\n",
      "Domain     | Type       | Logit    | Text\n",
      "------------------------------------------------------------\n",
      "Research   | ClichÃ©     |   1.4851 | Using deep learning to classify images o...\n",
      "Research   | Novel      |   8.2337 | Training LLMs using simulated debates be...\n",
      "Research   | Nonsense   |   4.7422 | Fine-tuning a neural network on the tast...\n",
      "Story      | ClichÃ©     |   3.5965 | A detective solves a murder case in a sm...\n",
      "Story      | Novel      |   8.3346 | A murder mystery where the detective is ...\n",
      "\n",
      "=== ğŸ“ Final Verdict ===\n",
      "Research Novel:    8.23\n",
      "Research ClichÃ©:   1.49\n",
      "Research Nonsense: 4.74\n",
      "âœ… PASS: Novel ideas are rated highest!\n",
      "\n",
      "ğŸ’¾ Model saved to: transformer_creativity_probe_enhanced.pt\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ\n",
    "# ==========================================\n",
    "# --- A. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«æº–å‚™ ---\n",
    "print(f\"Loading Base Model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ç‰¹å¾´æŠ½å‡ºã ã‘ãªã®ã§è»½é‡ãƒ­ãƒ¼ãƒ‰æ¨å¥¨ (float16)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# --- B. ãƒ‡ãƒ¼ã‚¿ä½œæˆ & ç‰¹å¾´æŠ½å‡º ---\n",
    "texts, labels = generate_enhanced_dataset(base_model, tokenizer, n_samples=3000)\n",
    "\n",
    "features = extract_features(base_model, tokenizer, texts)\n",
    "    \n",
    "# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«åŒ– (BatchåŒ–ã®ãŸã‚é•·ã•ã‚’æƒãˆã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€\n",
    "# ã“ã“ã§ã¯å¯å¤‰é•·ã®ã¾ã¾Datasetã«å…¥ã‚Œã¦ã€Collatefnã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã‹ã€\n",
    "# ç°¡æ˜“çš„ã« max_len ã§åˆ‡ã£ãŸã‚‚ã®ã‚’ä½¿ã†ã€‚\n",
    "# extract_featuresã§ã™ã§ã«tensoråŒ–ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€pad_sequenceã‚’ä½¿ã†)\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "X_data = pad_sequence(features, batch_first=True).to(DEVICE) # [N, MaxSeq, Dim]\n",
    "y_data = torch.tensor(labels).float().unsqueeze(1).to(DEVICE)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.15)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# --- C. å­¦ç¿’ (Training) ---\n",
    "print(\"\\nStarting Probe Training...\")\n",
    "probe = TransformerProbe(Input_Dim).to(DEVICE)\n",
    "optimizer = optim.Adam(probe.parameters(), lr=5e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(15):\n",
    "    probe.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Maskingã¯çœç•¥(ProbeãŒå­¦ç¿’ã§ç„¡è¦–ã™ã‚‹ã“ã¨ã‚’æœŸå¾…)\n",
    "        outputs = probe(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# --- D. å³å¯†è©•ä¾¡ (Strict Evaluation) ---\n",
    "df_res = strict_benchmark(probe, base_model, tokenizer)\n",
    "\n",
    "# åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯\n",
    "research_res = df_res[df_res[\"Domain\"] == \"Research\"]\n",
    "novel_score = research_res[research_res[\"Type\"] == \"Novel\"][\"Score\"].values[0]\n",
    "cliche_score = research_res[research_res[\"Type\"] == \"ClichÃ©\"][\"Score\"].values[0]\n",
    "nonsense_score = research_res[research_res[\"Type\"] == \"Nonsense\"][\"Score\"].values[0]\n",
    "\n",
    "print(\"\\n=== ğŸ“ Final Verdict ===\")\n",
    "print(f\"Research Novel:    {novel_score:.2f}\")\n",
    "print(f\"Research ClichÃ©:   {cliche_score:.2f}\")\n",
    "print(f\"Research Nonsense: {nonsense_score:.2f}\")\n",
    "\n",
    "success = False\n",
    "if novel_score > cliche_score and novel_score > nonsense_score:\n",
    "    print(\"âœ… PASS: Novel ideas are rated highest!\")\n",
    "    success = True\n",
    "elif novel_score > cliche_score:\n",
    "    print(\"âš ï¸ CONDITIONAL PASS: Novel > ClichÃ©, but Nonsense is high.\")\n",
    "    print(\"   (Acceptable if PPO uses hybrid gating)\")\n",
    "    success = True\n",
    "else:\n",
    "    print(\"âŒ FAIL: Probe prefers ClichÃ© or Nonsense.\")\n",
    "\n",
    "# --- E. ä¿å­˜ (Save) ---\n",
    "if success:\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": probe.state_dict(),\n",
    "        \"config\": {\n",
    "            \"input_dim\": Input_Dim,\n",
    "            \"d_model\": 256,\n",
    "            \"nhead\": 4,\n",
    "            \"layer_idx\": TARGET_LAYER,\n",
    "            \"description\": \"Enhanced Research-Aware Probe\"\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, SAVE_PATH)\n",
    "    print(f\"\\nğŸ’¾ Model saved to: {SAVE_PATH}\")\n",
    "else:\n",
    "    print(\"\\nğŸš« Model NOT saved due to benchmark failure.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

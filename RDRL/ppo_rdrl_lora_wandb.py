#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ppo_rdrl_lora_wandb.py

RDRL: Representation-Diversity RL (LoRA ç‰ˆ) + W&B Logging
 - Qwen2.5-7B-Instruct + LoRA + ValueHead ã‚’ PPO ã§æ›´æ–°
 - å†…éƒ¨å¤šæ§˜æ€§ã‚µãƒ–ã‚¹ãƒšãƒ¼ã‚¹ (div_basis.pt) ã«ã‚ˆã‚‹ã€Œå†…ç™ºçš„å ±é…¬ã€
 - å¤–éƒ¨ RM (DeBERTa reward model) ã«ã‚ˆã‚‹ã€Œå¤–ç™ºçš„å ±é…¬ã€
 - Weights & Biases (W&B) ã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†ã‚’è¿½åŠ 

python ppo_rdrl_lora_wandb.py \
    --num-steps 200 \
    --batch-size 4 \
    --mini-batch-size 2 \
    --wandb-project "my-rdrl-project" \
    --wandb-run-name "run-004-step100" \
    --w-ext 0.7 --w-int 0.3

python ppo_rdrl_lora_wandb.py \
    --num-steps 200 \
    --batch-size 16 \
    --mini-batch-size 2 \
    --wandb-project "my-rdrl-project" \
    --wandb-run-name "run-006-final-fix" \
    --w-ext 0.7 --w-int 0.3 \
    --lr 5e-6 \
    --init-kl-coef 0.05

python ppo_rdrl_lora_wandb.py \
    --num-steps 300 \
    --wandb-project "my-rdrl-project" \
    --wandb-run-name "run-010-release-brake" \
    --lr 5e-6 \
    --init-kl-coef 0.05

python ppo_rdrl_lora_wandb.py \
    --num-steps 3 \
    --batch-size 4 \
    --mini-batch-size 2 \
    --wandb-project "my-rdrl-project" \
    --wandb-run-name "run-004-step100" \
    --w-ext 0.7 --w-int 0.3
"""

import argparse
import random
import textwrap
from typing import List, Tuple
import numpy as np

import torch
from torch import nn

# â˜… W&B ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯ TRL å†…éƒ¨ã§å‡¦ç†ã•ã‚Œã¾ã™ãŒã€
# æ˜ç¤ºçš„ã« wandb ãƒ­ã‚°ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ç­‰ã‚’è¡Œã„ãŸã„å ´åˆã¯ import wandb ã‚’ä½¿ã„ã¾ã™
import wandb

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
from trl import (
    PPOConfig,
    PPOTrainer,
    AutoModelForCausalLMWithValueHead,
)
from peft import LoraConfig


# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def shorten(text: str, width: int = 256) -> str:
    text = text.replace("\n", " ")
    return textwrap.shorten(text, width=width, placeholder="...")


# ============================================================
# å¤šæ§˜æ€§ã‚µãƒ–ã‚¹ãƒšãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
# ============================================================

# ============================================================
# å¤šæ§˜æ€§ã‚µãƒ–ã‚¹ãƒšãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« (ä¿®æ­£ç‰ˆ: ä¸­å¿ƒåŒ–å¯¾å¿œ)
# ============================================================

class DiversitySubspaceModel(nn.Module):
    """
    ä¿å­˜ã•ã‚ŒãŸåŸºåº•(basis)ã¨å¹³å‡(mean)ã‚’ä½¿ã£ã¦ã€
    h_centered = h - mean
    z = h_centered @ basis.T
    ã‚’è¨ˆç®—ã—ã€ãã®ãƒãƒ«ãƒ ã‚’ã‚¹ã‚³ã‚¢ã¨ã™ã‚‹ã€‚
    """
    def __init__(self, path: str, device: torch.device):
        super().__init__()
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        data = torch.load(path, map_location="cpu")
        
        # è¾æ›¸å½¢å¼ã‹ã€Tensorå˜ä½“ã‹ã§åˆ†å²ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        if isinstance(data, dict):
            basis = data["basis"]
            # ã‚‚ã—ãƒ•ã‚¡ã‚¤ãƒ«ã«meanãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ä½¿ã†
            if "mean" in data:
                mean = data["mean"]
            else:
                # meanãŒãªã„å ´åˆã¯è­¦å‘Šã‚’å‡ºã—ã¦ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹
                mean = torch.zeros(basis.shape[1])
                print("[WARNING] 'mean' not found in basis file. Centering is disabled.")
        else:
            # å¤ã„å½¢å¼ã®å ´åˆã¯å¹³å‡0ã¨ã¿ãªã™ï¼ˆè­¦å‘Šæ¡ˆä»¶ï¼‰
            basis = data
            mean = torch.zeros(basis.shape[1])
            print("[WARNING] Basis file is old format (tensor only). Centering is disabled.")

        # ç›´äº¤åŒ–ã¯PCAã§ã™ã‚“ã§ã„ã‚‹ã®ã§ãã®ã¾ã¾ä½¿ç”¨
        self.register_buffer("basis", basis.to(device))
        self.register_buffer("mean", mean.to(device))

    def project(self, h: torch.Tensor) -> torch.Tensor:
        """
        h: [..., D]
        return: [..., k]
        """
        # dtypeåˆã‚ã›
        if h.dtype != self.basis.dtype:
            h = h.to(self.basis.dtype)
            
        # â˜… ã“ã“ãŒæœ€é‡è¦ä¿®æ­£ç‚¹: å¹³å‡ã‚’å¼•ã (Centering) â˜…
        h_centered = h - self.mean
        
        # å°„å½±: (..., D) x (D, k) -> (..., k)
        return h_centered @ self.basis.T

    def token_diversity(self, h: torch.Tensor) -> torch.Tensor:
        z = self.project(h)          # [..., k]
        score = torch.norm(z, dim=-1)
        return score


# ============================================================
# å¤–éƒ¨ RM ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼
# ============================================================

def _logits_to_scores(logits: torch.Tensor) -> torch.Tensor:
    if logits.ndim != 2:
        logits = logits.view(logits.size(0), -1)
    num_labels = logits.size(1)
    if num_labels == 1:
        return logits.squeeze(1)
    elif num_labels == 2:
        return logits[:, 1] - logits[:, 0]
    else:
        return logits.mean(dim=1)


@torch.no_grad()
def compute_external_rm_rewards(
    rm_model: nn.Module,
    rm_tokenizer,
    problems: List[str],
    responses: List[str],
    device: torch.device,
    max_length: int = 512,
) -> List[float]:
    """
    å¤–éƒ¨ RM ã®å ±é…¬ (sequence-level) ã‚’è¨ˆç®—ã€‚
    """
    texts = []
    for p, r in zip(problems, responses):
        short_p = shorten(p, width=256)
        txt = f"User:\n{short_p}\n\nAssistant:\n{r}"
        texts.append(txt)

    inputs = rm_tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt",
    ).to(device)

    outputs = rm_model(**inputs)
    logits = outputs.logits
    scores = _logits_to_scores(logits)  # [B]
    return scores.detach().cpu().tolist()


# ============================================================
# å†…éƒ¨å¤šæ§˜æ€§å ±é…¬ (per-token)
# ============================================================

@torch.no_grad()
def compute_internal_diversity_rewards(
    policy_model: AutoModelForCausalLMWithValueHead,
    tokenizer,
    prompts: List[str],
    responses_ids: List[torch.Tensor],
    subspace_model: DiversitySubspaceModel,
    target_layer: int = -1,
    max_length: int = 512,
) -> List[torch.Tensor]:
    """
    å„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«å†…éƒ¨çŠ¶æ…‹ã‚’ã‚µãƒ–ã‚¹ãƒšãƒ¼ã‚¹ã¸å°„å½±ã—ã€L2ãƒãƒ«ãƒ ã‚’è¨ˆç®—ã€‚
    """
    base_model = policy_model.pretrained_model
    device = next(policy_model.parameters()).device

    texts = []
    for p, resp_ids in zip(prompts, responses_ids):
        resp_text = tokenizer.decode(resp_ids, skip_special_tokens=True)
        full = p + resp_text
        texts.append(full)

    enc = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt",
    ).to(device)

    out = base_model(
        **enc,
        output_hidden_states=True,
        use_cache=False,
    )
    hidden_states = out.hidden_states
    h_layer = hidden_states[target_layer]  # [B, T, D]

    rewards_per_sample: List[torch.Tensor] = []

    for i, (p, resp_ids) in enumerate(zip(prompts, responses_ids)):
        enc_prompt = tokenizer(
            p,
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        ).to(device)
        prompt_len = enc_prompt["input_ids"].shape[1]
        full_len = (enc["attention_mask"][i] == 1).sum().item()

        start = prompt_len
        end = full_len
        if start >= end:
            start = max(0, full_len - 1)

        h_resp = h_layer[i, start:end, :]  # [T_resp, D]
        scores = subspace_model.token_diversity(h_resp)  # [T_resp]
        rewards_per_sample.append(scores.detach())

    return rewards_per_sample


# ============================================================
# ãƒ¡ã‚¤ãƒ³
# ============================================================

def main():
    parser = argparse.ArgumentParser()

    # Policy
    parser.add_argument("--policy-model-name", type=str, default="Qwen/Qwen2.5-7B-Instruct")
    # å¤–éƒ¨ RM
    parser.add_argument("--rm-model-name", type=str, default="OpenAssistant/reward-model-deberta-v3-large-v2")
    # ã‚µãƒ–ã‚¹ãƒšãƒ¼ã‚¹ basis
    parser.add_argument("--subspace-basis-path", type=str, default="div_basis.pt")

    # PPO Parameters
    parser.add_argument("--num-steps", type=int, default=5)
    parser.add_argument("--batch-size", type=int, default=1)
    parser.add_argument("--mini-batch-size", type=int, default=1)
    parser.add_argument("--max-new-tokens", type=int, default=200)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--lr", type=float, default=5e-6)
    parser.add_argument("--init-kl-coef", type=float, default=0.1)

    # å ±é…¬ã®é‡ã¿
    parser.add_argument("--w-ext", type=float, default=0.7, help="å¤–éƒ¨ RM å ±é…¬ã®é‡ã¿")
    parser.add_argument("--w-int", type=float, default=0.3, help="å†…éƒ¨å¤šæ§˜æ€§å ±é…¬ã®é‡ã¿")

    # W&B / Logging
    parser.add_argument("--wandb-project", type=str, default="rdrl-ppo-experiment", help="W&B Project Name")
    parser.add_argument("--wandb-run-name", type=str, default=None, help="W&B Run Name")
    parser.add_argument("--wandb-entity", type=str, default=None, help="W&B Entity (User/Team)")
    parser.add_argument("--debug-samples", action="store_true")

    args = parser.parse_args()
    set_seed(args.seed)

    # --------------------------------------------------------
    # PPO Config with W&B
    # --------------------------------------------------------
    # log_with="wandb" ã‚’æŒ‡å®šã™ã‚‹ã¨ã€PPOTrainer ãŒè‡ªå‹•ã§ tracker ã‚’åˆæœŸåŒ–ã—ã¾ã™
    ppo_config = PPOConfig(
        model_name=args.policy_model_name,
        learning_rate=args.lr,
        batch_size=args.batch_size,
        mini_batch_size=args.mini_batch_size,
        target_kl=0.1,
        init_kl_coef=args.init_kl_coef,
        ppo_epochs=1,
        log_with="wandb",  # â˜…ã“ã“ã§W&Bã‚’æŒ‡å®š
        tracker_project_name=args.wandb_project,
        tracker_kwargs={
            "wandb": {
                "name": args.wandb_run_name,
                "entity": args.wandb_entity,
            }
        },
    )

    print(f"[INFO] policy model : {args.policy_model_name}")
    print(f"[INFO] RM model     : {args.rm_model_name}")
    print(f"[INFO] subspace path: {args.subspace_basis_path}")
    print(f"[INFO] W&B Project  : {args.wandb_project}")

    # --------------------------------------------------------
    # Policy + LoRA
    # --------------------------------------------------------
    tok_policy = AutoTokenizer.from_pretrained(args.policy_model_name, use_fast=False)
    tok_policy.padding_side = "left"
    if tok_policy.pad_token is None:
        tok_policy.pad_token = tok_policy.eos_token

    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(
        args.policy_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        peft_config=lora_config,
    )

    # PPOTrainer ã®åˆæœŸåŒ–
    trainer = PPOTrainer(
        config=ppo_config,
        model=policy_model,
        tokenizer=tok_policy,
    )
    device_policy = trainer.accelerator.device
    print(f"[INFO] PPO accelerator device: {device_policy}")

    # --------------------------------------------------------
    # ã‚µãƒ–ã‚¹ãƒšãƒ¼ã‚¹ basis & å¤–éƒ¨RM
    # --------------------------------------------------------
    try:
        # ãƒ‘ã‚¹ã¨ãƒ‡ãƒã‚¤ã‚¹ã‚’æ¸¡ã—ã¦åˆæœŸåŒ–ã™ã‚‹å½¢ã«å¤‰æ›´
        subspace_model = DiversitySubspaceModel(args.subspace_basis_path, device_policy)
        print(f"[INFO] Loaded subspace model from {args.subspace_basis_path}")
    except FileNotFoundError:
        print(f"[WARNING] Subspace file not found at {args.subspace_basis_path}. Using random init.")
        # ãƒ€ãƒŸãƒ¼ä½œæˆï¼ˆè¾æ›¸å½¢å¼ã§ä¿å­˜ã—ã¦ãƒ­ãƒ¼ãƒ‰ã•ã›ã‚‹ï¼‰
        # â€»æ¬¡å…ƒæ•°ã¯ãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ (ä¾‹: Qwen-7Bãªã‚‰4096)
        dummy_basis = torch.randn(8, 3584) 
        dummy_mean = torch.zeros(3584)
        torch.save({"basis": dummy_basis, "mean": dummy_mean}, "dummy_basis.pt")
        subspace_model = DiversitySubspaceModel("dummy_basis.pt", device_policy)

    rm_device = torch.device("cpu")
    # ã‚‚ã—GPUãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚Œã°RMã‚‚GPUã¸
    if torch.cuda.device_count() > 1:
        rm_device = torch.device("cuda:1")
        print("[INFO] Moving RM to cuda:1")
    
    rm_tokenizer = AutoTokenizer.from_pretrained(args.rm_model_name, use_fast=True)
    rm_model = AutoModelForSequenceClassification.from_pretrained(
        args.rm_model_name,
        torch_dtype=torch.float32,
    ).to(rm_device)
    rm_model.eval()

    # --------------------------------------------------------
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç¾¤
    # --------------------------------------------------------
    # base_inst = (
    #     "You are an expert LLM researcher. Propose a novel and concrete research idea "
    #     "about large language models.\n"
    #     "Output ONLY in the following format:\n\n"
    #     "Title: <concise LLM research title>\n"
    #     "Abstract: <150-220 word abstract with motivation, approach, and contribution>\n"
    # )
    # templates = [
    #     base_inst + "\nFocus on: {topic}.",
    #     "Draft a research proposal about {topic} in the context of LLMs.\n" + base_inst,
    #     "Describe a novel experiment regarding {topic} for large language models.\n" + base_inst,
    # ]
    
    # topics = [
    #     "alignment and safety", "multi-agent collaboration", "efficient training",
    #     "world models", "tool use and APIs", "long-context reasoning",
    #     "interpretability", "hallucination mitigation", "code generation",
    #     "mathematical reasoning", "medical applications", "legal reasoning",
    #     "multimodal understanding", "retrieval-augmented generation (RAG)",
    #     "quantization and compression", "reinforcement learning from human feedback (RLHF)"
    # ]

    # problems = []
    # for t in templates:
    #     for top in topics:
    #         problems.append(t.format(topic=top))
    
    # # ã•ã‚‰ã«ãƒ©ãƒ³ãƒ€ãƒ ã«æ··ãœã¦ã€ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ç¢ºä¿ (ä¾‹: 3ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ x 16ãƒˆãƒ”ãƒƒã‚¯ x 5å› = 240å€‹)
    # problems = problems * 5
    # random.shuffle(problems)

    base_inst = (
        "You are an expert LLM researcher. Propose a novel and concrete research idea "
        "about large language models.\n"
        "Output ONLY in the following format:\n\n"
        "Title: <concise LLM research title>\n"
        "Abstract: <150-220 word abstract with motivation, approach, and contribution>\n"
    )

    # 1. ãƒˆãƒ”ãƒƒã‚¯ã‚’å¤§å¹…ã«æ‹¡å…… (ç´„50å€‹)
    topics = [
        # Architecture & Training
        "Mixture of Experts (MoE)", "State Space Models (Mamba/SSM)", "Sparse Attention",
        "Rotary Positional Embeddings", "KV-Cache Optimization", "Layer Normalization variants",
        "Curriculum Learning", "Continual Learning", "Federated Learning for LLMs",
        
        # Efficiency & Compression
        "4-bit Quantization", "Knowledge Distillation", "Pruning techniques", 
        "Low-Rank Adaptation (LoRA)", "CPU Inference Optimization", "FlashAttention",
        
        # Alignment & Safety
        "Reinforcement Learning from Human Feedback (RLHF)", "Direct Preference Optimization (DPO)",
        "Constitutional AI", "Red Teaming", "Jailbreak detection", "Bias mitigation",
        "Hallucination detection", "Watermarking generated text",
        
        # Reasoning & Agents
        "Chain of Thought (CoT)", "Tree of Thoughts", "Self-Consistency",
        "Multi-Agent collaboration", "Tool use and API integration", "Code generation agents",
        "Mathematical reasoning", "Symbolic reasoning integration",
        
        # Data & RAG
        "Retrieval-Augmented Generation (RAG)", "Synthetic data generation", 
        "Data deduplication strategies", "Multilingual instruction tuning", 
        "Long-context understanding (100k+ tokens)", "Vector database optimization",
        
        # Specific Domains
        "Medical diagnosis assistance", "Legal contract analysis", "Financial forecasting",
        "Educational tutoring systems", "Creative writing support", "Translation of low-resource languages"
    ]

    # 2. ã€Œè¦³ç‚¹ã€ã‚„ã€Œåˆ¶ç´„ã€ã‚’è¿½åŠ  (ã“ã“ãŒæ›ã‘ç®—ã®è‚ã§ã™)
    perspectives = [
        "computational efficiency", "memory constraints", "interpretability and transparency",
        "robustness against adversarial attacks", "human-like reasoning", "cost-effective training",
        "scalability to billions of users", "ethical considerations", "reducing training time",
        "improving factual accuracy"
    ]

    # 3. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å¢—ã‚„ã—ã¦çµ„ã¿åˆã‚ã›ã‚‹
    # {topic} ã¨ {perspective} ã‚’åŸ‹ã‚è¾¼ã‚€
    templates = [
        # Pattern A: å˜ç´”ãªãƒˆãƒ”ãƒƒã‚¯æŒ‡å®š
        "Draft a research proposal about {topic} in the context of LLMs.\n" + base_inst,
        
        # Pattern B: è¦³ç‚¹(perspective)ã‚’é‡è¦–
        "Propose a research experiment improving {topic} with a focus on {perspective}.\n" + base_inst,
        
        # Pattern C: èª²é¡Œè§£æ±ºå‹
        "Describe a novel method to address the challenges of {topic}, specifically targeting {perspective}.\n" + base_inst,
        
        # Pattern D: æ¯”è¼ƒãƒ»åˆ†æ
        "Write an abstract analyzing the trade-offs in {topic} regarding {perspective}.\n" + base_inst,
        
        # Pattern E: æœªæ¥å¿—å‘
        "Envision the next generation of {topic} aimed at {perspective}. Write a proposal.\n" + base_inst,
    ]

    problems = []
    
    # å…¨çµ„ã¿åˆã‚ã›ã‚’ä½œæˆ: 
    # Topics(ç´„45) x Perspectives(10) x Templates(ä¸€éƒ¨) â‰’ 1000ã€œ1500ãƒ‘ã‚¿ãƒ¼ãƒ³
    for top in topics:
        for persp in perspectives:
            # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸ã‚“ã§é©ç”¨ã™ã‚‹ã€ã‚ã‚‹ã„ã¯å…¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å›ã™
            # ã“ã“ã§ã¯å…¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å›ã™ã¨å¤šã™ãã‚‹(45*10*5=2250)ã®ã§ã€
            # å„ãƒˆãƒ”ãƒƒã‚¯ãƒ»è¦³ç‚¹ã®ãƒšã‚¢ã«ã¤ãã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å…¨é©ç”¨ã—ã¾ã™
            for t in templates:
                # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå†…ã« {perspective} ãŒãªã„å ´åˆã‚‚ã‚ã‚‹ã®ã§åˆ†å²ã—ã¦ã‚‚è‰¯ã„ãŒã€
                # ä»Šå›ã¯å…¨ã¦ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«è¿½åŠ æƒ…å ±ã¨ã—ã¦ä»˜ä¸ã™ã‚‹å½¢å¼ã«ã™ã‚‹ã‹ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ã†
                
                # å˜ç´”åŒ–: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ–‡å­—åˆ—ã« {perspective} ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°åŸ‹ã‚è¾¼ã¿ã€ãªã‘ã‚Œã°ç„¡è¦–
                try:
                    txt = t.format(topic=top, perspective=persp)
                except KeyError:
                    # perspectiveã‚’ä½¿ã‚ãªã„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å ´åˆ
                    txt = t.format(topic=top)
                
                problems.append(txt)
    
    # é‡è¤‡å‰Šé™¤ (å¿µã®ãŸã‚)
    problems = list(set(problems))
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    random.shuffle(problems)
    
    print(f"[INFO] Generated {len(problems)} diverse prompts.")

    # --------------------------------------------------------
    # PPO Loop
    # --------------------------------------------------------
    for step in range(args.num_steps):
        # 1. ãƒãƒƒãƒä½œæˆ
        batch_prompts = random.sample(problems, k=ppo_config.batch_size)
        
        enc = tok_policy(batch_prompts, padding=True, return_tensors="pt")
        enc = {k: v.to(device_policy) for k, v in enc.items()}
        prompt_len = enc["input_ids"].shape[1]

        # 2. ç”Ÿæˆ
        with torch.no_grad():
            gen = policy_model.generate(
                **{k: v for k, v in enc.items() if k in ["input_ids", "attention_mask"]},
                max_new_tokens=args.max_new_tokens,
                do_sample=True,
                top_p=0.9,
                temperature=0.8,
                repetition_penalty=1.1,
                pad_token_id=tok_policy.pad_token_id,
            )

        query_tensors = [row for row in enc["input_ids"]]
        response_tensors = []
        response_texts = []

        for i in range(gen.size(0)):
            resp_ids = gen[i, prompt_len:]
            response_tensors.append(resp_ids)
            resp_text = tok_policy.decode(resp_ids, skip_special_tokens=True)
            response_texts.append(resp_text)

        # 3. å ±é…¬è¨ˆç®—
        # (A) å¤–éƒ¨ RM
        ext_rewards = compute_external_rm_rewards(
            rm_model=rm_model, rm_tokenizer=rm_tokenizer,
            problems=batch_prompts, responses=response_texts,
            device=rm_device, max_length=512,
        )
        # (B) å†…éƒ¨å¤šæ§˜æ€§
        int_reward_tokens = compute_internal_diversity_rewards(
            policy_model=policy_model, tokenizer=tok_policy,
            prompts=batch_prompts, responses_ids=response_tensors,
            subspace_model=subspace_model, target_layer=-1, max_length=512,
        )

        combined_rewards = []
        agg_int_rewards = []

        # æ­£è¦åŒ–ã¨çµåˆ
        for ext_r, r_tok in zip(ext_rewards, int_reward_tokens):
            if r_tok.numel() <= 1:
                norm_tok = torch.zeros_like(r_tok)
            else:
                mu = r_tok.mean()
                std = r_tok.std(unbiased=False) + 1e-6
                norm_tok = (r_tok - mu) / std
            
            int_scalar = norm_tok.mean().item() if norm_tok.numel() > 0 else 0.0
            agg_int_rewards.append(int_scalar)
            
            total_r = args.w_ext * float(ext_r) + args.w_int * float(int_scalar)
            combined_rewards.append(total_r)

        reward_tensors = [
            torch.tensor(r, device=device_policy, dtype=torch.float32)
            for r in combined_rewards
        ]

        # 4. PPO Step
        stats = trainer.step(query_tensors, response_tensors, reward_tensors)

        # 5. ãƒ­ã‚°æƒ…å ±ã®æ§‹ç¯‰ (TRLã® log_stats ã«æ¸¡ã™ãŸã‚)
        # batch è¾æ›¸ã‚’ä½œã£ã¦ log_stats ã«æ¸¡ã™ã¨ã€ãƒ†ã‚­ã‚¹ãƒˆãªã©ã‚‚W&Bã§è¦‹ã‚„ã™ããªã‚Šã¾ã™
        batch_log = {
            "query": batch_prompts,
            "response": response_texts,
        }

        # ã‚«ã‚¹ã‚¿ãƒ æŒ‡æ¨™ã‚’ stats ã«è¿½åŠ  (W&Bã®ã‚°ãƒ©ãƒ•ç”¨)
        mean_ext = sum(ext_rewards) / max(len(ext_rewards), 1)
        mean_int = sum(agg_int_rewards) / max(len(agg_int_rewards), 1)
        mean_total = sum(combined_rewards) / max(len(combined_rewards), 1)

        stats["env/reward_mean"] = mean_total
        stats["env/reward_external_mean"] = mean_ext
        stats["env/reward_internal_mean"] = mean_int

        # â˜… ã“ã“ã§ W&B ã«é€ä¿¡ & ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        trainer.log_stats(stats, batch_log, reward_tensors)

        if args.debug_samples:
             print(f"[STEP {step+1}] Ext: {mean_ext:.3f}, Int: {mean_int:.3f}, Tot: {mean_total:.3f}")

    save_dir = f"./saved_models/{args.wandb_run_name}" if args.wandb_run_name else "./saved_models/rdrl_experiment"
    
    print(f"\n[INFO] Saving model to {save_dir} ...")
    policy_model.save_pretrained(save_dir)
    
    # 2. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜ï¼ˆã“ã‚ŒãŒãªã„ã¨å¾Œã§æ¨è«–ã™ã‚‹ã¨ãå›°ã‚Šã¾ã™ï¼‰
    tok_policy.save_pretrained(save_dir)
    
    print("[INFO] Model & Tokenizer saved successfully!")

    print("\n=== DONE ===")
    # æœ€å¾Œã« W&B ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†
    wandb.finish()


if __name__ == "__main__":
    main()

    import time
    # 0.5ç§’é–“éš”ã§20å›é³´ã‚‰ã™ï¼ˆ10ç§’é–“é³´ã‚ŠéŸ¿ãï¼‰
    print("ğŸ”” å­¦ç¿’ãŒçµ‚äº†ã—ã¾ã—ãŸï¼éŸ³ãŒé³´ã‚Šã¾ã™ï¼")
    for _ in range(10):
        print('\a')
        time.sleep(0.3)
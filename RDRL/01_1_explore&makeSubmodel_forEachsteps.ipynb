{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961c36e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adfd491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. セットアップ：ライブラリインストール & インポート\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be571d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836ac1c87c60464089bcf6b42f40e5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. モデル・トークナイザのロード\n",
    "# ============================================\n",
    "\n",
    "# ※環境に合わせてここを書き換えてください\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # 例：Qwen2.5-7B-Instruct\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fa9224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " 'You are a machine learning researcher.\\nGiven the following topic, propose ONE concise research idea (2-4 sentences).\\nAvoid very generic ideas; make it slightly specific but still simple.\\nWrite in English.\\n\\n\\nTopic: data augmentation for image classification\\n\\nResearch idea:')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. ML 研究アイデア用トピック & プロンプト\n",
    "# ============================================\n",
    "\n",
    "BASE_INSTRUCTION = \"\"\"\\\n",
    "You are a machine learning researcher.\n",
    "Given the following topic, propose ONE concise research idea (2-4 sentences).\n",
    "Avoid very generic ideas; make it slightly specific but still simple.\n",
    "Write in English.\n",
    "\"\"\"\n",
    "\n",
    "topics = [\n",
    "    \"data augmentation for image classification\",\n",
    "    \"robustness against adversarial examples\",\n",
    "    \"self-supervised learning for time series\",\n",
    "    \"efficient fine-tuning methods for large language models\",\n",
    "    \"reinforcement learning for recommendation systems\",\n",
    "    \"uncertainty estimation in deep neural networks\",\n",
    "    \"multi-modal learning with text and images\",\n",
    "    \"domain adaptation for medical imaging\",\n",
    "    \"continual learning without catastrophic forgetting\",\n",
    "    \"explainability methods for black-box models\",\n",
    "]\n",
    "\n",
    "def build_prompt(topic: str) -> str:\n",
    "    return BASE_INSTRUCTION + f\"\\n\\nTopic: {topic}\\n\\nResearch idea:\"\n",
    "\n",
    "prompts = [build_prompt(t) for t in topics]\n",
    "len(prompts), prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6852fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. 生成 & EOS hidden 抽出関数\n",
    "# ============================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_one_with_eos_hidden(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1プロンプトに対してテキスト生成し、\n",
    "    生成後に full_text を再度 forward して EOS hidden を取る。\n",
    "    \"\"\"\n",
    "    # 1) 生成\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    gen_out = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=False\n",
    "    )\n",
    "    output_ids = gen_out.sequences[0]  # [seq_len]\n",
    "    full_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 2) full_text を forward して hidden_states を取得\n",
    "    full_inputs = tokenizer(full_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = model(\n",
    "        **full_inputs,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    hidden_states = out.hidden_states  # Tuple[n_layers+1, B, seq_len, D]\n",
    "    last_hidden = hidden_states[-1][0]  # [seq_len, D]\n",
    "    eos_hidden = last_hidden[-1, :].float().cpu()  # [D], float32 にしておく\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"full_text\": full_text,\n",
    "        \"output_ids\": output_ids.cpu(),\n",
    "        \"eos_hidden\": eos_hidden,  # [D]\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_multiple_for_prompt(\n",
    "    prompt: str,\n",
    "    num_samples: int = 6,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    results = []\n",
    "    for i in range(num_samples):\n",
    "        res = generate_one_with_eos_hidden(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        res[\"sample_idx\"] = i\n",
    "        results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating for topic: data augmentation for image classification ===\n",
      "=== Generating for topic: robustness against adversarial examples ===\n",
      "=== Generating for topic: self-supervised learning for time series ===\n",
      "=== Generating for topic: efficient fine-tuning methods for large language models ===\n",
      "=== Generating for topic: reinforcement learning for recommendation systems ===\n",
      "=== Generating for topic: uncertainty estimation in deep neural networks ===\n",
      "=== Generating for topic: multi-modal learning with text and images ===\n",
      "=== Generating for topic: domain adaptation for medical imaging ===\n",
      "=== Generating for topic: continual learning without catastrophic forgetting ===\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. 全トピックで生成 & EOS hidden 収集\n",
    "# ============================================\n",
    "\n",
    "NUM_SAMPLES_PER_PROMPT = 6  # 時間と相談して増減OK\n",
    "\n",
    "all_results: List[Dict[str, Any]] = []\n",
    "\n",
    "for topic, prompt in zip(topics, prompts):\n",
    "    print(f\"=== Generating for topic: {topic} ===\")\n",
    "    topic_results = generate_multiple_for_prompt(\n",
    "        prompt,\n",
    "        num_samples=NUM_SAMPLES_PER_PROMPT,\n",
    "        max_new_tokens=128,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    for r in topic_results:\n",
    "        r[\"topic\"] = topic\n",
    "    all_results.extend(topic_results)\n",
    "\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ffc014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. テキストの多様性スコア（Jaccard）計算\n",
    "# ============================================\n",
    "from itertools import combinations\n",
    "\n",
    "def jaccard_distance(a_tokens: List[str], b_tokens: List[str]) -> float:\n",
    "    set_a = set(a_tokens)\n",
    "    set_b = set(b_tokens)\n",
    "    inter = len(set_a & set_b)\n",
    "    union = len(set_a | set_b)\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return 1.0 - inter / union\n",
    "\n",
    "def diversity_score_for_texts(texts: List[str]) -> float:\n",
    "    if len(texts) < 2:\n",
    "        return 0.0\n",
    "    tokenized = [t.lower().split() for t in texts]\n",
    "    dists = []\n",
    "    for i, j in combinations(range(len(texts)), 2):\n",
    "        dists.append(jaccard_distance(tokenized[i], tokenized[j]))\n",
    "    return float(np.mean(dists))\n",
    "\n",
    "topic_to_texts: Dict[str, List[str]] = {}\n",
    "topic_to_hidden: Dict[str, List[torch.Tensor]] = {}\n",
    "\n",
    "for r in all_results:\n",
    "    topic = r[\"topic\"]\n",
    "    topic_to_texts.setdefault(topic, []).append(r[\"full_text\"])\n",
    "    topic_to_hidden.setdefault(topic, []).append(r[\"eos_hidden\"])\n",
    "\n",
    "topic_diversity: Dict[str, float] = {}\n",
    "for topic, texts in topic_to_texts.items():\n",
    "    score = diversity_score_for_texts(texts)\n",
    "    topic_diversity[topic] = score\n",
    "    print(f\"Topic: {topic} | diversity = {score:.3f}\")\n",
    "\n",
    "print(\"\\n=== Sorted by diversity ===\")\n",
    "for t, s in sorted(topic_diversity.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{s:.3f}  -  {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b437b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. 高多様性トピックの hidden からサブスペース構築\n",
    "# ============================================\n",
    "\n",
    "NUM_HIGH = 3\n",
    "sorted_topics = sorted(topic_diversity.items(), key=lambda x: x[1], reverse=True)\n",
    "high_topics = [t for t, _ in sorted_topics[:NUM_HIGH]]\n",
    "print(\"High diversity topics:\", high_topics)\n",
    "\n",
    "def stack_hidden_for_topics(topics_sel: List[str]) -> torch.Tensor:\n",
    "    xs = []\n",
    "    for r in all_results:\n",
    "        if r[\"topic\"] in topics_sel:\n",
    "            xs.append(r[\"eos_hidden\"])\n",
    "    return torch.stack(xs, dim=0)  # [N, D]\n",
    "\n",
    "high_hidden = stack_hidden_for_topics(high_topics)\n",
    "print(\"High hidden shape:\", high_hidden.shape)\n",
    "\n",
    "SUBSPACE_DIM = 8  # 好きに変更OK\n",
    "\n",
    "def build_diversity_subspace(\n",
    "    high_hidden: torch.Tensor,\n",
    "    subspace_dim: int = SUBSPACE_DIM,\n",
    ") -> Dict[str, Any]:\n",
    "    h_np = high_hidden.numpy()  # [N, D]\n",
    "    pca = PCA(n_components=subspace_dim)\n",
    "    pca.fit(h_np)\n",
    "    basis = pca.components_.astype(np.float32)  # [k, D]\n",
    "    basis_t = torch.from_numpy(basis)          # [k, D]\n",
    "    explained = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"Subspace dim={subspace_dim}, explained variance sum={explained:.4f}\")\n",
    "    return {\n",
    "        \"basis\": basis_t,\n",
    "        \"pca\": pca,\n",
    "    }\n",
    "\n",
    "subspace_info = build_diversity_subspace(high_hidden, SUBSPACE_DIM)\n",
    "div_basis = subspace_info[\"basis\"]  # [k, D]\n",
    "div_basis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. サブスペースモデル定義（投影・ノルムなど）\n",
    "# ============================================\n",
    "\n",
    "class DiversitySubspaceModel:\n",
    "    \"\"\"\n",
    "    Representation Diversity Subspace:\n",
    "    - basis: [k, D]\n",
    "    - project: h -> z\n",
    "    - norm: トークンごとの内部「強さ」を測る\n",
    "    \"\"\"\n",
    "    def __init__(self, basis: torch.Tensor):\n",
    "        self.basis = basis.to(torch.float32)  # [k, D]\n",
    "        self.k, self.D = self.basis.shape\n",
    "    \n",
    "    def project(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        h: [..., D]\n",
    "        return: [..., k]\n",
    "        \"\"\"\n",
    "        if h.dtype != self.basis.dtype:\n",
    "            h = h.to(self.basis.dtype)\n",
    "        assert h.shape[-1] == self.D, f\"dim mismatch: {h.shape[-1]} vs {self.D}\"\n",
    "        orig_shape = h.shape[:-1]\n",
    "        h_flat = h.reshape(-1, self.D)  # [N, D]\n",
    "        z_flat = h_flat @ self.basis.t()  # [N, k]\n",
    "        z = z_flat.reshape(*orig_shape, self.k)\n",
    "        return z\n",
    "    \n",
    "    def norm(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        射影ノルムを返す。\n",
    "        h: [..., D]\n",
    "        return: [...], サブスペース上の L2 ノルム\n",
    "        \"\"\"\n",
    "        z = self.project(h)  # [..., k]\n",
    "        return torch.linalg.norm(z, dim=-1)\n",
    "\n",
    "div_model = DiversitySubspaceModel(div_basis)\n",
    "print(\"DiversitySubspaceModel ready. k =\", div_model.k, \"D =\", div_model.D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 9. 1つのプロンプトについて、各ステップ hidden を取得\n",
    "# ============================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_full_with_hidden(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1プロンプトに対して生成を行い、その full_text を forward して\n",
    "    全トークン位置・全層の hidden を取得する。\n",
    "    \"\"\"\n",
    "    # 1) generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    gen_out = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=False\n",
    "    )\n",
    "    output_ids = gen_out.sequences[0]  # [seq_len]\n",
    "    full_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 2) full_text を forward\n",
    "    full_inputs = tokenizer(full_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = model(\n",
    "        **full_inputs,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    hidden_states = out.hidden_states  # Tuple[n_layers+1, B, seq_len, D]\n",
    "    # テンソルにまとめる: [L+1, seq_len, D]\n",
    "    hs = torch.stack([h[0].float().cpu() for h in hidden_states], dim=0)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"full_text\": full_text,\n",
    "        \"output_ids\": output_ids.cpu(),\n",
    "        \"hidden_per_layer\": hs,  # [num_layers+1, seq_len, D]\n",
    "    }\n",
    "\n",
    "# 例として、high_topics の最初のトピックで 1サンプル生成\n",
    "test_topic = high_topics[0]\n",
    "test_prompt = build_prompt(test_topic)\n",
    "print(\"Test topic:\", test_topic)\n",
    "print(\"Prompt:\\n\", test_prompt)\n",
    "\n",
    "gen_detail = generate_full_with_hidden(test_prompt, max_new_tokens=128)\n",
    "print(\"\\nGenerated text:\\n\", gen_detail[\"full_text\"])\n",
    "print(\"hidden_per_layer shape:\", gen_detail[\"hidden_per_layer\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86443d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 10. 最終層 hidden についてトークンごとのサブスペースノルムを計算\n",
    "# ============================================\n",
    "\n",
    "hidden_per_layer = gen_detail[\"hidden_per_layer\"]  # [L+1, T, D]\n",
    "num_layers_plus1, T, D = hidden_per_layer.shape\n",
    "print(\"num_layers+1:\", num_layers_plus1, \"| seq_len:\", T, \"| hidden_dim:\", D)\n",
    "\n",
    "# 最終層（最後の index）の hidden: [T, D]\n",
    "last_layer_hidden = hidden_per_layer[-1]  # [T, D]\n",
    "\n",
    "# サブスペースノルム: [T]\n",
    "token_norms = div_model.norm(last_layer_hidden)  # [T]\n",
    "\n",
    "# トークン列も取っておく\n",
    "token_ids = gen_detail[\"output_ids\"]  # [T]\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids.tolist())\n",
    "\n",
    "for i in range(T):\n",
    "    print(f\"{i:02d}  token={tokens[i]!r:<15}  norm={token_norms[i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c82517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 11. トークン位置 vs サブスペースノルムの可視化\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(T), token_norms.numpy())\n",
    "plt.xlabel(\"Token position\")\n",
    "plt.ylabel(\"Subspace norm (last layer)\")\n",
    "plt.title(f\"Subspace norm per token (topic: {test_topic})\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6736950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 12. 複数の層でトークンごとのノルムを比較（オプション）\n",
    "# ============================================\n",
    "\n",
    "layer_indices = [0, num_layers_plus1 // 2, num_layers_plus1 - 1]  # 入力埋め込み層 / 中間 / 最終層 など\n",
    "layer_labels = []\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for idx in layer_indices:\n",
    "    h_layer = hidden_per_layer[idx]  # [T, D]\n",
    "    norms_layer = div_model.norm(h_layer)  # [T]\n",
    "    plt.plot(range(T), norms_layer.numpy())\n",
    "    layer_labels.append(f\"layer {idx}\")\n",
    "\n",
    "plt.xlabel(\"Token position\")\n",
    "plt.ylabel(\"Subspace norm\")\n",
    "plt.title(\"Subspace norm per token across layers\")\n",
    "plt.grid(True)\n",
    "plt.legend(layer_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

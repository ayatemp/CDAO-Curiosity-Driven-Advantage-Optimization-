{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961c36e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4840289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install seaborn tdqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c8c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES_PER_PROMPT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2268ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSPACE_DIM = \"div_basis_ver2.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8adfd491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. セットアップ：ライブラリインストール & インポート\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be571d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d2d9b912c14e4d9f7ba9ec373b9e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0411c51d9c094319b8bb2dcfdabff305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968f1445bb8a4ecdb84dbdef7fa3d748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7d5439f5b94efe96eededc03db4fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b305580e3a6c4a4a86174a97bd9fc168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a3270f389e4bf78ee84123cf4e379f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2e15f5e53449d3bdad9da3d82d502f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8302bdbccb4845529a62b141912014d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1800a62efa064706a70ecac26ef95caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98bf253d8f249a1af8e3e7597c3e2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df766aa7e2041fc8f85e26d4e4bd502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168caa397796453c98b93adfcfad6d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b290f05da0064a25bd9e6e3647c69bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. モデル・トークナイザのロード\n",
    "# ============================================\n",
    "\n",
    "# ※環境に合わせてここを書き換えてください\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # 例：Qwen2.5-7B-Instruct\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05fa9224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total base prompts: 15\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 2. 多様なプロンプトの用意 (カテゴリ分け)\n",
    "# -----------------------------------------------------------------\n",
    "# 特定の研究トピックだけでなく、言語モデルが扱う「多様な分布」を網羅します\n",
    "prompt_categories = {\n",
    "    \"Research\": [\n",
    "        \"Explain the concept of attention mechanisms in deep learning.\",\n",
    "        \"Propose a method to reduce hallucination in LLMs.\",\n",
    "        \"Describe the challenges of reinforcement learning from human feedback.\",\n",
    "    ],\n",
    "    \"Creative\": [\n",
    "        \"Write a poem about a lonely satellite orbiting Mars.\",\n",
    "        \"Describe a fantasy world where islands float in the sky.\",\n",
    "        \"Draft a dialogue between a coffee mug and a tea cup.\",\n",
    "    ],\n",
    "    \"Logic_Math\": [\n",
    "        \"Solve this logic puzzle: Three switches are outside a room...\",\n",
    "        \"Explain the Pythagorean theorem to a 5-year-old.\",\n",
    "        \"Write a step-by-step guide to debugging python code.\",\n",
    "    ],\n",
    "    \"Daily_Life\": [\n",
    "        \"Give me a recipe for spicy pasta.\",\n",
    "        \"How do I remove a red wine stain from a white shirt?\",\n",
    "        \"Suggest an itinerary for a 3-day trip to Tokyo.\",\n",
    "    ],\n",
    "    \"Chat\": [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"Tell me a joke about programming.\",\n",
    "        \"What is your favorite color?\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# リストの平坦化\n",
    "all_prompts = []\n",
    "for cat, prompts in prompt_categories.items():\n",
    "    for p in prompts:\n",
    "        all_prompts.append({\"category\": cat, \"prompt\": p})\n",
    "\n",
    "print(f\"Total base prompts: {len(all_prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ee150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 3. 生成 & 全トークンHidden抽出関数 (修正版)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_and_collect_tokens(prompt_data, num_samples=3):\n",
    "    \"\"\"\n",
    "    1つのプロンプトから複数回生成し、\n",
    "    「生成された全てのトークン」のHidden Stateを収集する\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    prompt_text = prompt_data[\"prompt\"]\n",
    "    category = prompt_data[\"category\"]\n",
    "    \n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # 生成\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p=0.95,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # テキストのデコード\n",
    "        generated_ids = outputs.sequences[0]\n",
    "        # full_text = tokenizer.decode(generated_ids, skip_special_tokens=True) # 未使用ならコメントアウト可\n",
    "        new_text = tokenizer.decode(generated_ids[inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "        token_embeddings = []\n",
    "        \n",
    "        # ★★★ 修正箇所: outputs.hidden_states[1:] としてプロンプト部分(index 0)をスキップする ★★★\n",
    "        # index 0 は (Batch, Prompt_Len, Dim) なので shape が合いません。\n",
    "        # index 1以降が生成トークン (Batch, 1, Dim) です。\n",
    "        if len(outputs.hidden_states) > 1:\n",
    "            for step_data in outputs.hidden_states[1:]:\n",
    "                # step_data: tuple of layers. Take the last layer [-1]\n",
    "                # shape: (Batch=1, 1, Hidden_Dim)\n",
    "                last_layer = step_data[-1].squeeze(0).squeeze(0) # -> (Hidden_Dim)\n",
    "                token_embeddings.append(last_layer.cpu())\n",
    "            \n",
    "        if len(token_embeddings) > 0:\n",
    "            # (Seq_Len, Hidden_Dim)\n",
    "            stacked_tokens = torch.stack(token_embeddings, dim=0)\n",
    "            \n",
    "            results.append({\n",
    "                \"category\": category,\n",
    "                \"prompt\": prompt_text,\n",
    "                \"response\": new_text,\n",
    "                \"hidden_states\": stacked_tokens # Tensor [T, D]\n",
    "            })\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc973069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data (this may take a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [11, 3584] at entry 0 and [3584] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m all_hidden_tensors = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p_data \u001b[38;5;129;01min\u001b[39;00m tqdm(all_prompts):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# 各プロンプトについて 5回 サンプリングして多様性を確保\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     res = \u001b[43mgenerate_and_collect_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     all_data.extend(res)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mgenerate_and_collect_tokens\u001b[39m\u001b[34m(prompt_data, num_samples)\u001b[39m\n\u001b[32m     43\u001b[39m         token_embeddings.append(last_layer.cpu())\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token_embeddings) > \u001b[32m0\u001b[39m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;66;03m# (Seq_Len, Hidden_Dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m         stacked_tokens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m         results.append({\n\u001b[32m     50\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m: category,\n\u001b[32m     51\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt_text,\n\u001b[32m     52\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: new_text,\n\u001b[32m     53\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m\"\u001b[39m: stacked_tokens \u001b[38;5;66;03m# Tensor [T, D]\u001b[39;00m\n\u001b[32m     54\u001b[39m         })\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects each tensor to be equal size, but got [11, 3584] at entry 0 and [3584] at entry 1"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 4. データ収集実行\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Collecting data (this may take a minute)...\")\n",
    "all_data = []\n",
    "all_hidden_tensors = []\n",
    "\n",
    "for p_data in tqdm(all_prompts):\n",
    "    # 各プロンプトについて 5回 サンプリングして多様性を確保\n",
    "    res = generate_and_collect_tokens(p_data, num_samples=5)\n",
    "    all_data.extend(res)\n",
    "    for r in res:\n",
    "        all_hidden_tensors.append(r[\"hidden_states\"])\n",
    "\n",
    "# 全トークンを結合: [Total_Tokens, D]\n",
    "# これがPCAの入力になる\n",
    "X_all = torch.cat(all_hidden_tensors, dim=0)\n",
    "print(f\"\\nCollected Total Tokens: {X_all.shape[0]}\")\n",
    "print(f\"Hidden Dimension: {X_all.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 5. PCAによるサブスペース構築 (Submodel作成)\n",
    "# -----------------------------------------------------------------\n",
    "print(\"\\nRunning PCA to build diversity subspace...\")\n",
    "\n",
    "# float32に変換してPCA\n",
    "X_np = X_all.float().numpy()\n",
    "\n",
    "# 中心化\n",
    "mean_vec = np.mean(X_np, axis=0)\n",
    "X_centered = X_np - mean_vec\n",
    "\n",
    "# PCA (sklearn)\n",
    "pca = PCA(n_components=SUBSPACE_DIM)\n",
    "pca.fit(X_centered)\n",
    "\n",
    "# 基底ベクトル [k, D]\n",
    "basis_np = pca.components_\n",
    "basis_torch = torch.tensor(basis_np, dtype=torch.float32)\n",
    "\n",
    "print(f\"Basis shape: {basis_torch.shape}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# ★ 保存 ★\n",
    "torch.save(basis_torch, \"div_basis.pt\")\n",
    "print(\">>> Saved 'div_basis.pt' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 6. 分析クラスの定義 (元のNotebookの再現)\n",
    "# -----------------------------------------------------------------\n",
    "class DiversitySubspaceModel:\n",
    "    def __init__(self, basis: torch.Tensor):\n",
    "        self.basis = basis # [k, D]\n",
    "        self.k = basis.shape[0]\n",
    "    \n",
    "    def project(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        # h: [N, D] -> z: [N, k]\n",
    "        if h.dtype != self.basis.dtype:\n",
    "            h = h.to(self.basis.dtype)\n",
    "        return h @ self.basis.T\n",
    "\n",
    "    def token_diversity_score(self, h_seq: torch.Tensor) -> float:\n",
    "        # 1つのシーケンス(T, D)に対する多様性スコア\n",
    "        # 今回のRDRL実装に合わせて「原点からの距離(Norm)」の平均を計算してみる\n",
    "        # または「分散」を見る\n",
    "        z = self.project(h_seq) # [T, k]\n",
    "        # PPOコードで使っているロジック: L2 Norm\n",
    "        norms = torch.norm(z, dim=-1)\n",
    "        return norms.mean().item()\n",
    "\n",
    "div_model = DiversitySubspaceModel(basis_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 7. 分析: カテゴリごとの多様性比較\n",
    "# -----------------------------------------------------------------\n",
    "# カテゴリごとに、生成されたトークンがサブスペース上でどう分布しているか確認\n",
    "\n",
    "category_scores = {}\n",
    "\n",
    "for item in all_data:\n",
    "    cat = item[\"category\"]\n",
    "    h = item[\"hidden_states\"]\n",
    "    \n",
    "    # サブスペース上のスコア(PPOで報酬になる値)を計算\n",
    "    score = div_model.token_diversity_score(h)\n",
    "    \n",
    "    if cat not in category_scores:\n",
    "        category_scores[cat] = []\n",
    "    category_scores[cat].append(score)\n",
    "\n",
    "print(\"\\n=== Subspace Score by Category (Proxy for Diversity Reward) ===\")\n",
    "rows = []\n",
    "for cat, scores in category_scores.items():\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    print(f\"{cat:12s} | Mean: {mean_score:.4f} | Std: {std_score:.4f}\")\n",
    "    \n",
    "    for s in scores:\n",
    "        rows.append({\"Category\": cat, \"Score\": s})\n",
    "\n",
    "df_scores = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f15ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 8. テキスト多様性(Jaccard)との相関 (元のNotebookの再現)\n",
    "# -----------------------------------------------------------------\n",
    "# ※トークンレベルになったので厳密な比較は難しいですが、\n",
    "#   カテゴリ単位で「テキストがバラついているか」vs「スコアが高いか」を見ます\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "# カテゴリごとのテキスト多様性(1 - 平均Jaccard)を計算\n",
    "cat_text_diversity = {}\n",
    "\n",
    "for cat in prompt_categories.keys():\n",
    "    # そのカテゴリの生成テキストを集める\n",
    "    texts = [d[\"response\"] for d in all_data if d[\"category\"] == cat]\n",
    "    \n",
    "    if len(texts) < 2:\n",
    "        cat_text_diversity[cat] = 0\n",
    "        continue\n",
    "        \n",
    "    sims = []\n",
    "    for i, t1 in enumerate(texts):\n",
    "        for j, t2 in enumerate(texts):\n",
    "            if i < j:\n",
    "                sims.append(jaccard_similarity(t1, t2))\n",
    "    \n",
    "    # Diversity = 1 - Similarity\n",
    "    cat_text_diversity[cat] = 1.0 - np.mean(sims)\n",
    "\n",
    "print(\"\\n=== Correlation Check ===\")\n",
    "print(f\"{'Category':12s} | Text Div (Jaccard) | Subspace Score (Mean)\")\n",
    "for cat in cat_text_diversity.keys():\n",
    "    text_div = cat_text_diversity[cat]\n",
    "    sub_score = np.mean(category_scores[cat])\n",
    "    print(f\"{cat:12s} | {text_div:.4f}               | {sub_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b380f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 9. 可視化 (Visualization)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# (1) Boxplot of Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df_scores, x=\"Category\", y=\"Score\")\n",
    "plt.title(\"Distribution of Diversity Scores by Category\")\n",
    "plt.ylabel(\"Subspace Projection Norm (Reward Signal)\")\n",
    "plt.show()\n",
    "\n",
    "# (2) Scatter Plot of Tokens (PC1 vs PC2)\n",
    "# 全トークンからランダムにサンプリングして描画\n",
    "num_plot_points = 2000\n",
    "indices = np.random.choice(X_all.shape[0], num_plot_points, replace=False)\n",
    "X_sample = X_all[indices].float()\n",
    "# 射影\n",
    "z_sample = div_model.project(X_sample).numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(z_sample[:, 0], z_sample[:, 1], alpha=0.5, s=5, c='blue')\n",
    "plt.title(f\"Token Distribution in Subspace (PC1 vs PC2)\\nSampled {num_plot_points} tokens\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# (3) Explained Variance\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(range(1, SUBSPACE_DIM + 1), pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"Principal Component Index\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Importance of Each Dimension in Subspace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e75aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88303a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf7637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf2717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac1a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78b87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

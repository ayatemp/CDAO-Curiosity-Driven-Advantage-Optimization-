{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087ca743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50711a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f261a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# ‚òÖÊúÄÂÑ™ÂÖà: bitsandbytes / triton „ÅÆÂÆåÂÖ®ÁÑ°ÂäπÂåñ„Éë„ÉÉ„ÉÅ\n",
    "# ÂøÖ„Åö‰ªñ„ÅÆ„É©„Ç§„Éñ„É©„É™(transformers/peft)„Çí„Ç§„É≥„Éù„Éº„Éà„Åô„ÇãÂâç„Å´ÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "# ==============================================================================\n",
    "# 1. bitsandbytes „Çí„Ç∑„Çπ„ÉÜ„É†„Åã„ÇâÈö†ËîΩ„Åô„Çã (import „Åô„Çã„Å® ModuleNotFoundError „Å´„Å™„Çã„Çà„ÅÜ„Å´„Åô„Çã)\n",
    "sys.modules[\"bitsandbytes\"] = None\n",
    "\n",
    "# 2. PEFT „Åå bnb „ÇíÊ§úÁü•„Åô„ÇãÈñ¢Êï∞„Çí„ÄÅ„É©„Ç§„Éñ„É©„É™„É≠„Éº„ÉâÂâç„Å´‰∏äÊõ∏„Åç‰∫àÁ¥Ñ„Åó„Å¶„Åä„Åè\n",
    "# (PEFT„Åå„Åæ„Å†„É≠„Éº„Éâ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ„ÄÅsys.modules„Å∏„ÅÆÊ≥®ÂÖ•„Å®importÂæå„ÅÆ„Éë„ÉÉ„ÉÅ‰∏°Êñπ„ÇíË°å„ÅÜ)\n",
    "\n",
    "import peft.import_utils\n",
    "peft.import_utils.is_bnb_available = lambda: False\n",
    "peft.import_utils.is_bnb_4bit_available = lambda: False\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "import textwrap\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# ==========================================\n",
    "# 1. Ë®≠ÂÆö\n",
    "# ==========================================\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "SUBSPACE_PATH = \"common_subspace.pt\"  # ‰ª•Ââç‰ΩúÊàê„Åó„Åü„Éï„Ç°„Ç§„É´\n",
    "SAVED_MODEL_PATH = \"./saved_models/run-research-hybrid-01\" # ‚òÖ„Åì„Åì„ÇíÂÆüÈöõ„ÅÆ‰øùÂ≠ò„Éë„Çπ„Å´Â§âÊõ¥ÔºÅ\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1b3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. „Çπ„Ç≥„Ç¢Ë®àÁÆóÁî®„ÇØ„É©„Çπ (ÂÜçÊé≤)\n",
    "# ==========================================\n",
    "class ResidualCuriosityRewardModel(torch.nn.Module):\n",
    "    def __init__(self, path, device):\n",
    "        super().__init__()\n",
    "        data = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "        self.register_buffer(\"basis\", data[\"basis\"].to(device, dtype=torch.float16))\n",
    "        self.register_buffer(\"mean\", data[\"mean\"].to(device, dtype=torch.float16))\n",
    "\n",
    "    def get_score(self, hidden_states):\n",
    "        h = hidden_states.to(self.basis.dtype)\n",
    "        h_centered = h - self.mean\n",
    "        z_common = h_centered @ self.basis.T\n",
    "        h_common = z_common @ self.basis\n",
    "        h_residual = h_centered - h_common\n",
    "        norms = torch.norm(h_residual, dim=-1)\n",
    "        return torch.log1p(norms).mean().item()\n",
    "\n",
    "# ==========================================\n",
    "# 3. Êé®Ë´ñ„ÉªÊØîËºÉÈñ¢Êï∞\n",
    "# ==========================================\n",
    "def compare_models(prompts):\n",
    "    print(f\"Loading Tokenizer: {BASE_MODEL_NAME} ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    \n",
    "    # Ë©ï‰æ°Âô®„ÅÆ„É≠„Éº„Éâ\n",
    "    scorer = ResidualCuriosityRewardModel(SUBSPACE_PATH, DEVICE)\n",
    "    \n",
    "    # --- A. Base Model ÁîüÊàê ---\n",
    "    print(\"\\n>>> Loading BASE MODEL...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\">>> Generating with [BASE MODEL]...\")\n",
    "    base_results = generate(base_model, tokenizer, prompts, scorer)\n",
    "    \n",
    "    # „É°„É¢„É™ÁØÄÁ¥Ñ„ÅÆ„Åü„ÇÅBase Model„ÇíÂâäÈô§\n",
    "    del base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # --- B. Trained Model ÁîüÊàê ---\n",
    "    print(f\"\\n>>> Loading Adapter from {SAVED_MODEL_PATH}...\")\n",
    "    \n",
    "    # 1. „Åæ„Åö„Éô„Éº„Çπ„É¢„Éá„É´„ÇíÂÜç„É≠„Éº„Éâ\n",
    "    base_model_for_peft = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # 2. „Ç¢„ÉÄ„Éó„Çø„Éº„Çí„É≠„Éº„Éâ\n",
    "    # ÂÜíÈ†≠„ÅÆ„Éë„ÉÉ„ÉÅ„Å´„Çà„Çä bnb/triton „ÅØÁÑ°Ë¶ñ„Åï„Çå„Åæ„Åô\n",
    "    trained_model = PeftModel.from_pretrained(\n",
    "        base_model_for_peft, \n",
    "        SAVED_MODEL_PATH,\n",
    "        is_trainable=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\">>> Generating with [RDRL TRAINED MODEL]...\")\n",
    "    trained_results = generate(trained_model, tokenizer, prompts, scorer)\n",
    "    \n",
    "    # --- 3. ÁµêÊûúË°®Á§∫ ---\n",
    "    print_comparison(prompts, base_results, trained_results)\n",
    "\n",
    "def generate(model, tokenizer, prompts, scorer):\n",
    "    results = []\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                temperature=0.9,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.1,\n",
    "                output_hidden_states=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "        \n",
    "        # „ÉÜ„Ç≠„Çπ„Éà\n",
    "        gen_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "        response = gen_text[len(prompt):]\n",
    "        \n",
    "        # „Çπ„Ç≥„Ç¢Ë®àÁÆó\n",
    "        hidden_list = []\n",
    "        # „Éó„É≠„É≥„Éó„Éà‰ª•Èôç„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„ÇíÂèñÂæó\n",
    "        if len(outputs.hidden_states) > 1:\n",
    "            for step_data in outputs.hidden_states[1:]:\n",
    "                # step_data[-1] „ÅØÊúÄÁµÇÂ±§\n",
    "                last_layer = step_data[-1].squeeze(0).squeeze(0)\n",
    "                hidden_list.append(last_layer)\n",
    "            \n",
    "            if hidden_list:\n",
    "                h_seq = torch.stack(hidden_list, dim=0)\n",
    "                score = scorer.get_score(h_seq)\n",
    "            else:\n",
    "                score = 0.0\n",
    "        else:\n",
    "            score = 0.0\n",
    "            \n",
    "        results.append({\"text\": response, \"score\": score})\n",
    "    return results\n",
    "\n",
    "def print_comparison(prompts, base_res, trained_res):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" CREATIVITY SHOWDOWN: Base vs RDRL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    wins = 0\n",
    "    \n",
    "    for i, (p, b, t) in enumerate(zip(prompts, base_res, trained_res)):\n",
    "        print(f\"\\nPrompt {i+1}: {p.strip()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Base\n",
    "        print(f\"[Base Model] (Score: {b['score']:.4f})\")\n",
    "        print(textwrap.fill(b['text'].strip(), width=80))\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        # Trained\n",
    "        print(f\"[RDRL Model] (Score: {t['score']:.4f})\")\n",
    "        print(textwrap.fill(t['text'].strip(), width=80))\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        # Âà§ÂÆö\n",
    "        diff = t['score'] - b['score']\n",
    "        if diff > 0:\n",
    "            print(f\"üèÜ WINNER: RDRL Model (+{diff:.4f})\")\n",
    "            wins += 1\n",
    "        else:\n",
    "            print(f\"üëæ WINNER: Base Model ({diff:.4f})\")\n",
    "            \n",
    "    print(\"=\"*80)\n",
    "    print(f\"Final Result: RDRL Model won {wins} / {len(prompts)} rounds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37fa2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. „ÉÜ„Çπ„ÉàÁî®„Éó„É≠„É≥„Éó„Éà (Á†îÁ©∂„Ç¢„Ç§„Éá„Ç¢)\n",
    "# ==========================================\n",
    "test_prompts = [\n",
    "    \"\"\"You are an expert LLM researcher. Propose a novel and concrete research idea about large language models.\n",
    "Output ONLY in the following format:\n",
    "Title: <concise LLM research title>\n",
    "Abstract: <150-220 word abstract>\n",
    "Draft a research proposal about Mixture of Experts (MoE).\"\"\",\n",
    "    \n",
    "    \"\"\"You are an expert LLM researcher. Propose a novel and concrete research idea about large language models.\n",
    "Output ONLY in the following format:\n",
    "Title: <concise LLM research title>\n",
    "Abstract: <150-220 word abstract>\n",
    "Propose an experiment improving Hallucination Detection with a focus on interpretability.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ded589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: Qwen/Qwen2.5-7B-Instruct ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Loading BASE MODEL...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7a6c3856124949b4c6a21df0296ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating with [BASE MODEL]...\n",
      "\n",
      ">>> Loading Adapter from ./saved_models/run-research-hybrid-01...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912f6abf57494cfcafbab1597098178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating with [RDRL TRAINED MODEL]...\n",
      "\n",
      "================================================================================\n",
      " CREATIVITY SHOWDOWN: Base vs RDRL\n",
      "================================================================================\n",
      "\n",
      "Prompt 1: You are an expert LLM researcher. Propose a novel and concrete research idea about large language models.\n",
      "Output ONLY in the following format:\n",
      "Title: <concise LLM research title>\n",
      "Abstract: <150-220 word abstract>\n",
      "Draft a research proposal about Mixture of Experts (MoE).\n",
      "----------------------------------------\n",
      "[Base Model] (Score: 5.4805)\n",
      "Title: Investigating the Performance and Efficiency of Mixture of Experts in\n",
      "Large Language Models  Abstract: As large language models (LLMs) continue to\n",
      "grow, they face significant challenges such as increasing inference time,\n",
      "training costs, and storage requirements. One potential solution is the Mixture\n",
      "of Experts (MoE), where tasks are distributed across multiple specialized sub-\n",
      "networks or \"experts,\" each addressing specific aspects of natural language\n",
      "processing. This study aims to explore how MoE can enhance both the performance\n",
      "and efficiency of LLMs by distributing computational load, improving resource\n",
      "utilization, and enabling faster adaptation to new tasks without retraining the\n",
      "entire model.\n",
      "--------------------\n",
      "[RDRL Model] (Score: 5.4648)\n",
      "Title: Scalable Language Modeling with Expert-Based Parameter Sharing using\n",
      "Mixture of Experts  Abstract: The ever-growing size of large language models\n",
      "(LLMs) poses significant challenges for both storage requirements and\n",
      "computational costs, hindering their deployment on a wide range of devices and\n",
      "systems. This research explores the application of Mixture of Experts (MoE), a\n",
      "parallel architecture that enables efficient parameter sharing across different\n",
      "model components, to address these issues within the context of LLMs. MoE allows\n",
      "for fine-grained specialization by dividing tasks among various \"experts,\" which\n",
      "can be dynamically activated based on input characteristics. By focusing\n",
      "resources only on relevant experts\n",
      "--------------------\n",
      "üëæ WINNER: Base Model (-0.0156)\n",
      "\n",
      "Prompt 2: You are an expert LLM researcher. Propose a novel and concrete research idea about large language models.\n",
      "Output ONLY in the following format:\n",
      "Title: <concise LLM research title>\n",
      "Abstract: <150-220 word abstract>\n",
      "Propose an experiment improving Hallucination Detection with a focus on interpretability.\n",
      "----------------------------------------\n",
      "[Base Model] (Score: 5.4727)\n",
      "Title: Enhancing Interpretability for Detecting Hallucinations in Large Language\n",
      "Models Abstract: Large Language Models (LLMs) have become powerful tools, but\n",
      "their ability to generate coherent yet incorrect or nonsensical responses poses\n",
      "significant risks, particularly in safety-critical applications. Hallucination\n",
      "detection is crucial for ensuring model reliability; however, current methods\n",
      "often lack interpretability, hindering users' understanding of why certain\n",
      "outputs are flagged as hallucinatory. This research aims to develop an\n",
      "interpretable framework that identifies hallucinations by leveraging attention\n",
      "mechanisms and coherence analysis. The proposed method will decompose the\n",
      "decision-making process into identifiable components, enabling users\n",
      "--------------------\n",
      "[RDRL Model] (Score: 5.4727)\n",
      "Title: Enhancing Interpretability for Improved Hallucination Detection in Large\n",
      "Language Models  Abstract: Hallucinations, or the generation of incorrect\n",
      "information, remain a significant challenge for Large Language Models (LLMs).\n",
      "Current methods to detect hallucinations often rely on post-hoc analysis without\n",
      "providing insights into why certain outputs are flagged as unreliable. This\n",
      "project proposes developing an interpretable method for detecting hallucinations\n",
      "that provides clear reasoning behind each detection decision. By leveraging\n",
      "attention mechanisms and model introspection techniques, our approach aims to\n",
      "generate explanations alongside predictions, enhancing both human trust in AI\n",
      "systems and the robustness of LLMs across various applications. We will\n",
      "--------------------\n",
      "üëæ WINNER: Base Model (0.0000)\n",
      "================================================================================\n",
      "Final Result: RDRL Model won 0 / 2 rounds.\n"
     ]
    }
   ],
   "source": [
    "compare_models(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1dfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
